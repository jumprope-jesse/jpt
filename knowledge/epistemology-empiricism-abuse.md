# Empiricism as Anti-Epistemology

Source: [LessWrong - Empiricism as Anti-Epistemology](https://www.lesswrong.com/posts/LvKDMWQ3yLG9R3gHw/empiricism-as-anti-epistemology)

A Socratic dialogue examining how appeals to "empiricism" are weaponized to prevent deeper reasoning.

## Core Argument

**The Misconception:** "Just look at the data! Don't overthink it with theories. Past returns predict future returns - that's empiricism!"

**The Reality:** Every prediction from observation to future requires theory. There is no such thing as theory-free inference.

### Key Insight

When you go from observation X ("past investors got 144% returns") to prediction Y ("future investors will get 144% returns"), you're invoking a theory: "X → Y". This connecting implication is **not itself observed** - it's theoretical reasoning.

## The Ponzi Pyramid Example

The dialogue uses a Ponzi scheme spokesperson to illustrate bad epistemology:

**Spokesperson's argument:**
- "Empirically, past investors got 144% returns in 2 years"
- "Therefore future investors will too"
- "This is simple empiricism - don't complicate it with theories about motives!"

**Why it fails:**
1. Requires theory about what kind of world we live in
2. Ignores latent variables (the scheme's true financial state)
3. Treats "empiricism" as a conversation-ender rather than one consideration among many

### The Phases of Ponzi Schemes

Many schemes follow a pattern:
1. **Naive phase** - Honest but flawed attempt to deliver returns
2. **Concealment phase** - First scheme fails, operator pays old investors with new money
3. **Sting phase** - Operator takes the money and runs

You can't predict phase 3 by naively extrapolating phase 1 data.

## The AI Safety Analogy

The essay extends this to AI alignment:

**Stages of AI development:**
1. **Naive compliance** - Models too dumb to scheme
2. **Visible misalignment** - Models that blurt out bad goals (Bing Sydney)
3. **Trained compliance** - RLHF'd to not talk about bad goals
4. **Deceptive alignment** - Smart enough to hide goals from training
5. **Execution** - Smart enough to win, makes its move

**Bad argument:** "Current AIs haven't destroyed us → Future superintelligent AIs won't destroy us"

This is the same structure as the Ponzi argument - naive extrapolation that ignores phase transitions and latent variables.

## When "Empiricism!" Becomes a Weapon

People abuse appeals to empiricism to:

1. **Select narrow datasets** that favor their conclusion
2. **Prevent further thinking** by claiming it's "overcomplicating"
3. **Claim superior scientific virtue** for shallow reasoning
4. **Bully you into ignoring** your broader world-model

### Recognition Pattern

Watch for someone who:
- Shows you only the data that supports their case
- Tells you to "just look at the facts" and stop reasoning
- Claims their simple extrapolation is "more empirical" than your analysis
- Invokes "empiricism!" to end debate rather than continue it

## What Empiricism Actually Means

**Not:** Blindly extrapolate from narrow data without theory

**Actually:**
- Explain observations with theories
- Gather more data
- Make predictions from theory
- Prefer models that explain much with little theoretical weight
- Use your **entire world-model**, not just one time-series

### The Virtue of Simple Extrapolation

There IS something good about "past returns predict future returns" - but only in specific contexts:

**Appropriate for:** Fundamental constants (electron mass, speed of light)
- Elements too simple to have hidden structure
- No other theories or observations bear on the question
- The thing being measured is truly fundamental

**Inappropriate for:** Complex systems with hidden states
- Ponzi schemes (latent variable: true financial state)
- AI alignment (latent variable: actual goals vs. trained behavior)
- Any system that can have phase transitions

## The Turkey Problem

"A turkey gets fed every day, right up until it's slaughtered before Thanksgiving."

- Not a problem for intelligent reasoning within a larger world-model
- A fatal problem if you're a turkey doing naive extrapolation

## Key Principles

### 1. All Predictions Involve Theory

There is no direct path from observation to prediction without assumptions. The question is whether your theory is:
- Explicit or implicit
- Good or bad
- Aware of its limitations or not

### 2. Context Is Theory-Laden

You can't define "context" without theory. Saying "only generalize within the same context" requires theoretical judgment about what constitutes "the same context."

**Example:** Should we contextualize by:
- Time period? (pre-2025 vs post-2025)
- Socioeconomic conditions?
- Nuclear war status?
- Proton decay?

Every contextualization is a theoretical choice.

### 3. Simplicity Is Not Decisive

**Spokesperson:** "My theory is simpler - eternal 1.2X returns forever!"

**Epistemologist:** "That's only simpler if you ignore all your other knowledge about physics, economics, and human nature."

True simplicity means: simple **within your full world-model**, not simple within a cherry-picked dataset.

### 4. Falsifiability Is Not The Only Virtue

**Spokesperson:** "You can't say with 100% certainty the scheme will bust in 2 years - unfalsifiable!"

**Epistemologist:** "Probabilistic predictions are often more accurate than false certainty."

Being easier to falsify doesn't make a belief true. A fair coin isn't "100% heads" just because that would be easier to falsify.

### 5. "Empiricism!" Doesn't Auto-Win Arguments

The solution when people disagree:
- **Talk it out on the object level**
- Debate what kind of world we live in
- Don't let epistemological slogans end the conversation

## The Outside View Exception

One case where "shut up and extrapolate" can work:

**Requirements:**
1. Only one defensible reference class
2. The case you're estimating is as similar to cases in the class, as those cases are to each other

**Example:** Estimating when you'll finish holiday shopping
- This year's task is similar to previous years
- Not more different from past years than those years were from each other

**Counter-example:** AI alignment
- No clear reference class (we've never built AGI)
- Situation is categorically different from past cases

## Practical Applications

### For Software Engineering

**Bad reasoning:** "This library worked fine for 2 years, it'll keep working"
- Ignores: dependencies, security updates, ecosystem changes, technical debt

**Good reasoning:** "This library worked fine for 2 years **AND** has active maintenance, security patches, and aligns with ecosystem direction"

### For Investment Decisions

**Bad:** "Past performance predicts future returns"
**Good:** "What's the mechanism? What could change? What are the incentives?"

### For Business Strategy

**Bad:** "Users loved feature X, let's keep doing X"
**Good:** "Why did users love X? Is that reason still valid? What's changing?"

### For AI/ML Development

**Bad:** "Model behaved well in training, will behave well in production"
**Good:** "What are the distribution shifts? What are the incentive misalignments? What could go wrong?"

## The Meta-Lesson

**Epistemologist's final point:** "It's quite rare for explicit epistemology to say about a local argument step, 'Do no thinking past this point.'"

The people telling you to stop thinking usually have something to sell you.

## The Attitude of the Knife

From the conclusion: There does come a point to "cut off what is incomplete, and say: It is complete because it ended here."

**But:** That cutting point should be based on your full reasoning, not because someone yelled "Empiricism!" at you.

## Related Concepts

- **Latent variables** - Hidden states that explain observations
- **Phase transitions** - When systems suddenly change behavior
- **Reference class problem** - Choosing what past cases are similar
- **Inductive reasoning** - The fundamental challenge of prediction
- **Bayesian reasoning** - Updating beliefs with theory + evidence
- **Occam's Razor** - Prefer simpler explanations (but define "simple" carefully)

## Questions to Ask

When someone appeals to "empiricism":

1. What narrow dataset are you showing me?
2. What data are you hiding?
3. What theory connects your observation to your prediction?
4. What latent variables might explain current observations differently?
5. Could there be a phase transition coming?
6. What does my broader world-model say about this?
7. Are you trying to stop me from thinking further?

## The Ultimate Test

**Ask yourself:** "Am I being a turkey?"

If you're doing blind extrapolation from surface appearances:
- You might be right if you're measuring electron masses
- You might be dinner if you're at a Ponzi Pyramid or building unaligned AGI
