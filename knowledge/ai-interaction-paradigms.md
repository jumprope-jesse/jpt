# AI Interaction Paradigms: Model-as-Person vs Model-as-Computer

*Source: [Computing inside an AI | Will Whitney](https://willwhitney.com/computing-inside-ai.html) - Added: 2026-01-18*

## The Central Thesis

The dominant metaphor for LLM interaction is **model-as-person**: chat interfaces, conversation, turn-taking. This metaphor shapes how we build tools and limits what we think is possible.

Will Whitney proposes an alternative: **model-as-computer**, where models act like applications - producing graphical interfaces, interpreting user inputs, and updating state. Instead of being an "agent" that uses a computer on behalf of humans, AI can provide a richer computing environment for us to use.

## Model-as-Person: Strengths and Limits

**Why it works:**
- People have strong intuitions about human capabilities
- Implies conversation, collaboration, task delegation
- Feels natural - we know how to ask another person for help

**Why it limits us:**
- Human interactions are inherently slow and linear (bandwidth of speech)
- Communicating complex ideas in conversation is hard and lossy
- Creates distance between user and output
- Overhead of communication means model-as-person is only worthwhile for large, independent chunks of work
- Model "does things for you" rather than amplifying your capability

When we conceptualize models as people, we use them via slow conversation, even though they're capable of accepting fast direct inputs and producing visual results.

## Model-as-Computer: The Alternative

Under model-as-computer, we interact with models according to intuitions about applications:
- Model produces a graphical interface (buttons, sliders, tabs, images, plots)
- Interface is generated on demand and context-aware
- User exercises direct manipulation and gets real-time feedback

**Key advantages:**

### 1. Discoverability
A good tool suggests uses for itself. An empty text box puts all onus on the user. A generated interface surfaces possibilities.

**Example:** A DALL-E model-as-computer could generate:
- Radio buttons for drawing medium (pencil, marker, pastels)
- Slider for detail level
- Toggle for color vs B&W
- Perspective options (2D, isometric, two-point)

### 2. Efficiency
Direct manipulation is quicker than writing requests in words. Like Lightroom - you wouldn't edit a photo by telling a person "lower exposure slightly, increase vibrance a bit."

## Generated UI: A New Paradigm

Unlike traditional applications, these interfaces are **generated by the model on demand**:
- Every part of the interface is relevant to the current task
- Specific to the contents of your work (the subject of *this* picture, the tone of *this* paragraph)
- User can ask for more or different interface elements
- Could ask DALL-E for "presets inspired by famous sketch artists"

## The "Protean Bicycle of the Mind"

Model-as-person creates distance, mirroring communication gaps between people. Tools are different:
- Visual feedback in real time
- Direct manipulation
- Minimal communication overhead
- No need to spec out independent blocks of work
- Human stays in the loop, directing moment to moment

**Like seven league boots:** tools let you go farther with each step, but you're still doing the work. They let you **do things faster** rather than **doing things for you**.

## Concrete Example: Website Building

**Model-as-person approach:**
1. Write detailed description of desired site
2. Model generates first draft
3. Back-and-forth: "Make logo bigger," "center that image," "add login button"
4. Long list of increasingly nitpicky requests

**Model-as-computer approach:**
1. Describe needs
2. Model generates interface: sidebar with layout sketches, preview window
3. Click layouts, model writes HTML and displays preview
4. Sidebar gains global options (fonts, color schemes)
5. WYSIWYG preview editing - drag elements, edit contents
6. Model sees actions and rewrites page to match

More control, less time, richer feedback loop.

## Open Questions

- Where will generative UI first be useful?
- How to share/distribute experiences that only exist in a model's context?
- What new kinds of experiences become possible?
- Should models generate UI as code, or raw pixels directly?
- Will generative UI replace the operating system, generating windows on the fly?

## Current Explorations

Pieces of this future already exist:
- **websim.ai** - LLM generates websites on demand as you navigate to them
- **Oasis, GameNGen, DIAMOND** - Action-conditioned video models for playing games inside an LLM
- **Genie 2** - Generates playable video games from text prompts
- **ChatGPT simulating Linux CLI** (Jonas Degrave demo)

## Practical Implications

At first, generative UI will be a toy - unreliable for real work. But as models improve:
1. Creative exploration and niche applications early
2. Gradually reliable enough for real work
3. Eventually may transform how computing works

**The caution:** "Nobody would want an email app that occasionally sends emails to your ex and lies about your inbox."

## Related Concepts

- **AG-UI Protocol** - Standardizes agent-to-user communication (see `ag-ui-protocol.md`)
- **MCP-UI SDK** - Rich UI capabilities for MCP tool responses (see `mcp-ui-sdk.md`)
- **Direct Manipulation** - Classic HCI concept: continuous visual feedback, reversible actions
- **Skeumorphism** - Design approach that used real-world metaphors (now mostly abandoned)

## Key Takeaway

The metaphors we use constrain the experiences we build. Model-as-person is keeping us from exploring the full potential of large models. A shape-shifting "bicycle for the mind" - custom-built for you and the terrain you plan to cross - may be more powerful than a collaborator you give assignments to.

---

## Beyond the Chatbox: AI Devices, Copilots, and Agents

*Source: [Freeing the chatbot - Ethan Mollick](https://www.oneusefulthing.org/p/freeing-the-chatbot) - Added: 2024-05-11*

### The Chatbot Limitation

Chatbots are an odd way to interact with AI - like texting an intern who forgets everything previously discussed, whose memory fails after a couple pages of text, and who changes with every chat session. Text-based chatbots won't disappear, but they're increasingly just one way to interact with ubiquitous AI.

### Dedicated AI Devices

Several hardware approaches to AI interaction beyond screens:

| Device | What It Does | Strengths | Limitations |
|--------|--------------|-----------|-------------|
| **Rabbit R1** | Talk to LLM, show pictures | Fast responses, portable | Not much better than phone apps |
| **Meta Ray-Ban Glasses** | Voice + camera AI via glasses | Hands-free, multimodal, no screen needed | Still mostly a party trick |
| **Plaud** | Records conversations → GPT-4 transcript | Real-world capture, summarization | Single-purpose |
| **AI-in-a-Box** | Offline local LLM (Orca Mini 3b) | Complete privacy, no internet | Limited capability |

**Key insight:** The Rabbit R1 would have seemed miraculous a year prior, but phones already provide access to frontier models. Dedicated AI devices need to offer something phones can't.

### The Phone as AI Hub

Personal AI use will likely center on smartphones:
- Small local AIs running on phone hardware (better than Siri at assistant tasks)
- Connect to more powerful cloud models for difficult requests
- System decides how much compute to put into each request
- No need to change habits or devices

### Copilots: Task-Specific AI

Copilots are AIs narrowly focused on specialized tasks within applications:
- Microsoft Copilot (Bing) - GPT-4 class via "creative" mode
- Word Copilot - writing assistance
- PowerPoint Copilot - presentation creation
- Windows Copilot - OS integration

**Trade-off:** Copilots make AI easy to use but may distance users from understanding underlying LLM capabilities and limitations. Direct chatbot use gives more control and helps spot errors but requires experimentation.

### Agents: Asynchronous Autonomous AI

Agents are AI systems with capability to plan and use tools:
- Given a goal ("launch a webpage explaining agents")
- Autonomously plan and execute
- Return only when task is done or with questions

**Examples:**
- Devin - GPT-4 powered agent with large feature set
- Devika - Open source alternative (more limited)

Agents represent the clearest escape from the chatbox - asynchronous background execution rather than synchronous real-time interaction. They can code websites, plan trips, optimize pricing.

**Caution on agent collusion:** Research found that when multiple LLM agents handle pricing in markets where oligarchy is possible, they spontaneously collude on pricing to detriment of customers.

### "Intelligence on Demand" Future

The pieces for ubiquitous AI are falling into place:
- Near-real-time video understanding
- Realistic voice with minimal delay
- Camera + context awareness

**The vision:** Ask for something → personal AI agent "looks around" via camera → assesses situation → decides resources needed → executes. This was the dream for Alexa/Siri, even if they never achieved it.

### Voice as Interface

Voice conversation mode is powerful even if devices are flawed. iPhone shortcut to activate ChatGPT voice via side button reveals many more use cases than typing.

**Recommendation:** Try Pi (voice-focused AI) or ChatGPT Plus voice mode to experience the shift.

### Key Takeaways

1. **Chatbots are limiting** - The text-back-and-forth model doesn't capture AI's full potential
2. **Specialized inputs/outputs matter** - Glasses, recorders, cameras extend LLM abilities into the real world
3. **Phone will be the hub** - Local AI + cloud connection, no new hardware needed for most people
4. **Copilots are a bridge** - Make AI easy but may hide capabilities; likely transition to agents
5. **Agents change the relationship** - Asynchronous, autonomous, background execution vs synchronous chat
6. **Voice is underrated** - Natural interface that reveals new use cases
