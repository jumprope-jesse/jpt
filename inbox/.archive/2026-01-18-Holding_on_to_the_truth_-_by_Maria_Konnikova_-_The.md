---
type: link
source: notion
url: https://substack.com/home/post/p-151951905
notion_type: Tech Deep Dive
tags: ['Running']
created: 2024-12-04T15:20:00.000Z
---

# Holding on to the truth - by Maria Konnikova - The Leap

## Content (from Notion)

I don’t remember when I last read George Orwell’s 1984, but it’s safe to say that it was quite some time ago. High school, most likely, maybe even junior high. Apart from the broad strokes, there’s much of the narrative that I surely don’t remember—the nuances of plot and character that come from reading a book at a more mature age. But there’s one moment that made such an impression on my young mind that I’ve thought about it many times since. (No, not the scene with the rats, even though that is the stuff of nightmares.) It’s the concept of BLACKWHITE.

Here is Orwell, writing about the idea:

> …there is need for an unwearying, moment-to-moment flexibility in the treatment of facts. The keyword here is BLACKWHITE. Like so many Newspeak words, this word has two mutually contradictory meanings. Applied to an opponent, it means the habit of impudently claiming that black is white, in contradiction of the plain facts. Applied to a Party member, it means a loyal willingness to say that black is white when Party discipline demands this. But it means also the ability to BELIEVE that black is white, and more, to KNOW that black is white, and to forget that one has ever believed the contrary.

I remember reading Orwell’s words and thinking that they were a step too far into the fictional domain. Sure, you could say black was white and vice versa under duress or to convince someone of your loyalty or to deceive someone willingly. But to believe and know that something that every single one of your senses—indeed, your entire life—tells you to be false, that seemed beyond the realm of possibility. Maybe the humans in the world of Big Brother could be made to alter their reality, but the humans in the real world couldn’t. Or could they?

I was pretty sure I would always be able to tell white from black—and then I started to study psychology. And the nature of belief. And of memory. And of how we process reality. And that’s when I realized that Orwell was not exaggerating, despite my deep-rooted desire to leave his musings on BLACKWHITE to the realm of make-believe.

As it turns out, our grasp on reality is far weaker than I’d ever thought possible. This is true for many, many reasons, but there’s one that feels particularly relevant today, and that’s our propensity to mistake the familiar with the true.

The anniversary edition cover of 1984, Penguin UK, 2009

We’ve all had that feeling: something sounds vaguely familiar, we think about it for a moment and say, yeah, that makes sense; I think I’ve heard that before. We may not remember where we heard it or when, but we are far more likely to think it’s credible if it feels known.

This conflation largely stems from two tendencies in our brains. First, when we process reality, we automatically assume things are true before verifying if they are, in fact, true. The reason is simple: to understand an idea, a concept, or an object, our brain first has to process it. And in order to process it, it has to assume that it’s correct before it can do the far more effortful job of verification. Second, we have a broad affinity for the familiar, in all aspects of our lives. Familiar foods. Familiar places. Familiar sounds. Familiar faces. Familiarity inspires trust and puts us at ease. So when we hear a notion that we’ve heard in the past, we are far more likely to assume that it’s true that if we’re hearing it for the first time. The more we hear it, the more likely we are to think it correct. And at some point, even if we once knew otherwise, we might actually start believing verifiably false statements to be true. This is called the illusory truth effect.

The illusory truth effect has been shown to hold even we originally heard a statement from a less-than-credible source; even if we have prior knowledge that goes against it; even if it contradicts common sense; and even if we’re told it’s likely not true when we first hear it. (A separate line of work on jury trials shows that even information that is immediately contradicted can influence our judgments: juries that are told to disregard something or that it’s actually not true will often still use the information in their deliberations.)

Traditionally, studies of the illusory truth effect have been done with relatively benign facts, such as the size of oceans. “The Pacific is larger than the Atlantic,” a true statement might read, while a false one might be, “The Atlantic is larger than the Pacific.” They can get a tiny bit trickier, like, “An ostrich is a bird that cannot fly and is the largest bird on earth” versus, “An emu is a bird that cannot fly and is the largest bird on earth.” (The first is true, the second, false.) And they can get more esoteric, like “chemosynthesis is the name of the process by which plants make their food.” (That’s photosynthesis, for those who remember high school bio, but doesn’t chemosynthesis sound vaguely correct? Indeed, a full 40% of participants in that particular study rated the statement as true.) You’re shown statements, asked if they are true or false, and then, at a later time—sometimes, hours, and sometimes, weeks—shown both statements you’ve seen before and new ones and asked to rate them once more. Repeatedly, researchers find that false facts that were shown before are far more likely to be rated as true upon repetition.

Image courtesy of the Decision Lab, https://thedecisionlab.com/biases/illusory-truth-effect

In 2018, a group from Yale, led by Gordon Pennycook (now at Cornell University), tested the phenomenon with news headlines. Would people misremember false headlines as true if they read them repeatedly? Over 1000 participants were shown twelve different headlines, six real and six fake. The real ones were taken from actual news, while the fake, from the fact-checking website Snopes. The fake headlines were purposefully partisan, half, in the Republican direction, and half, in the Democratic. They were also purposefully extreme—like, for instance, this one: “Trump to ban all TV shows that promote gay activity, starting with Empire, as President.”1 In one condition, the fake headlines even had fact checks appended to them, in the style of Twitter’s community notes. “Disputed by 3rd party fact-checkers,” a banner might read, along with a caution symbol.

Just like in studies that used real and fake facts, the participants were asked if they thought the headlines were true or not—and also, whether they’d consider sharing the story online. Then, they were questioned about demographics and political preferences. Finally, they were shown 24 additional headlines, the 12 from the first half of the study and 12 new ones (six fake, six real), and asked to do a truth assessment a second time.

While everyone correctly judged the real headlines to be more accurate than the fake ones, the second time a fake headline was seen, it was more likely to be seen as real than the first time—and more accurate than the new fake headlines. Indeed, twice as many participants said a fake piece of news was real the second time around. And it turns out that the disputed warnings were not particularly effective, either. Fake headlines with warnings that were seen a second time were rated higher on accuracy than new fake headlines without warnings. What’s more, warnings made real headlines seem less accurate, a sort of general distrust effect that carried over from one news type to another. Finally, politics mattered. If a fake news headline was concordant with your political leanings, you were more likely to see it as true the first time—and even more likely to see it as true the second time.

A second study repeated the design, with a week break in between the first stage and the repetition. Once again, perceived accuracy for fake news increased—and did so even for the stories that had fared the worst the first time around, regardless of party affiliation. For instance, take the fake news headline, “BLM Thug Protests President Trump with Selfie…Accidentally Shoots Himself in the Face.” Upon first exposure, 18.5% of Trump supporters rated this as accurate, while only 11.7% of Clinton supporters did so (remember, this is 2018, so political preferences are taken from the 2016 election). After a single repetition, the percentage of Trump supporters rating it as true rose to 35.5%—and Clinton supporters, 17.9%. That’s a huge increase after just one repetition. Now imagine what happens after two, three, four, countless repetitions. The more familiar, the more fluent, the more likely our gut will be to mark it as true. And the more likely our gut will be to lead us far astray.

The Leap is a reader-supported publication. To receive new posts and support my work, consider becoming a paid subscriber.

It's a scary proposition. I still remember the first time I went through an illusory truth study protocol myself. It seemed easy enough—and I knew exactly what the study was measuring. And then, a few weeks later, there it was. To my consternation and embarrassment, I found I had to fact-check something I’d always known. Sure, I could still tell black from white. But was the Pacific really bigger than the Atlantic or was it the other way around? The seed of doubt was there. Let me just do a quick search to make sure…

It was, quite honestly, frightening. We know memory is highly fallible. Wonky, even. That’s true even in the absence of any willful manipulation, any knowing attempt to fiddle with reality. Now, imagine what happens when we’re inundated. Most of the world isn’t as stark as black and white or even Pacific/Atlantic. It’s far closer to ostriches versus emus—or pieces of news that may or may not reflect actual events, if only we could remember. And on top of that, our fact-checking abilities are dwindling. These days, a simple search may well yield some hallucinatory bullshit courtesy of the latest AI LLM. An attempt to look through the news may yield insane hits long before any semblance of unbiased reporting. What’s a brain to do?

I don’t know the answer. But I do know that we have to be aware of our tendency to start believing in false statements just because we’ve heard them so many times before, of becoming more comfortable with something by sheer virtue of repetition, of seeing things that may be peripheral to our core areas of knowledge start to seem correct through the deceptive comfort of familiarity.

We have to be aware—and then, we have to fight back. No matter how overtaxed our resources may be, how strong the barrage of fake news may seem, how harmless it may feel to let little things slide in order to free our minds for Things That Matter. We have to fight for reality. Because if we don’t, we may discover one day that when we wake up, white is black, black is white, and no one is really sure how the change ever took root in the first place.

1

17.8% of people believed that one to be true the first time around.


