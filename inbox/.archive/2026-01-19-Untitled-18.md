---
type: link
source: notion
url: https://www.lesswrong.com/posts/bce63kvsAMcwxPipX/highlights-from-lex-fridman-s-interview-of-yann-lecun
notion_type: Tech Deep Dive
tags: ['Running']
created: 2024-03-14T03:41:00.000Z
---

# Untitled

## Overview (from Notion)
- AI Perspectives: LeCun critiques current AI models, emphasizing that autoregressive LLMs won't lead to superhuman intelligence. Consider how this might influence your work in software and product development.

- Embodiment Importance: He argues for the need for AI systems to be grounded in reality, which could inspire innovative approaches in your own projects that involve physical interactions or real-world applications.

- Diversity in AI: LeCun advocates for open-source AI to prevent power concentration in a few companies. This aligns with values of transparency and community engagement, relevant to your startup ethos.

- Human-AI Collaboration: The idea of AI amplifying human intelligence could lead to new business models or tools that empower users, enhancing productivity and creativity in your life and work.

- Skepticism of AI Doom Scenarios: His views challenge the prevalent fear of uncontrollable AI, suggesting a more balanced conversation around technology's role in society. This can inform your discussions with peers and family about AI's potential.

- Long-Term View on AGI: LeCun's belief in gradual progress towards AGI contrasts with more alarmist views; this perspective can help ground your expectations and strategic planning in your professional journey.

- Alternate Views: Some believe that AI could lead to quick, transformative changes in society. Engaging with these contrasting viewpoints can enhance critical thinking and innovation in your ventures.

- Cultural Impact of Technology: Reflect on how historical technological advancements, like the printing press, shaped society. This can inform your understanding of AI's potential effects on culture and education for your children.

## AI Summary (from Notion)
Yann LeCun argues that autoregressive LLMs won't achieve superhuman intelligence due to their limitations in understanding and reasoning. He emphasizes the importance of grounding AI in reality, advocates for open-source development to prevent bias, and believes AI can enhance human intelligence, similar to the impact of the printing press.

## Content (from Notion)

# Introduction

Yann LeCun is perhaps the most prominent critic of the “LessWrong view” on AI safety, the only one of the three "godfathers of AI" to not acknowledge the risks of advanced AI. So, when he recently appeared on the Lex Fridman podcast, I listened with the intent to better understand his position. LeCun came across as articulate / thoughtful[1]. Though I don't agree with it all, I found a lot worth sharing.

Most of this post consists of quotes from the transcript, where I’ve bolded the most salient points. There are also a few notes from me as well as a short summary at the end.

# Limitations of Autoregressive LLMs

> Lex Fridman (00:01:52) You've said that autoregressive LLMs are not the way we’re going to make progress towards superhuman intelligence. These are the large language models like GPT-4, like Llama 2 and 3 soon and so on. How do they work and why are they not going to take us all the way?

> Yann LeCun (00:02:47) For a number of reasons. The first is that there [are] a number of characteristics of intelligent behavior. For example, the capacity to understand the world, understand the physical world, the ability to remember and retrieve things, persistent memory, the ability to reason, and the ability to plan. Those are four essential characteristics of intelligent systems or entities, humans, animals. LLMs can do none of those or they can only do them in a very primitive way and they don’t really understand the physical world. They don’t really have persistent memory. They can’t really reason and they certainly can’t plan. And so if you expect the system to become intelligent without having the possibility of doing those things, you’re making a mistake. That is not to say that autoregressive LLMs are not useful. They’re certainly useful. That they’re not interesting, that we can’t build a whole ecosystem of applications around them… of course we can. But as a pass towards human-level intelligence, they’re missing essential components.

Checking some claims:

- An LLM training corpus is on order of 10^13 tokens. This seems about right: “Llama 2 was trained on 2.4T tokens and PaLM 2 on 3.6T tokens. GPT-4 is thought to have been trained on 4T tokens… Together AI introduced a 1 trillion (1T) token dataset called RedPajama in April 2023. A few days ago, it introduced a 30T token dataset”. That’s 2*10^12 - 3*10^13.
- 170,000 years to read this. Seems reasonable. Claude gives me an answer of 95,129 years (nonstop, for a 10^13 token corpus).
- Optical nerve carries ~20MB/s. One answer on StackExchange claims 8.75Mb/s (per eye), which equates to ~2MB/s.
- 10^15 bytes experienced by a four-year-old: 4 * 365 * 16 * 60 * 60 * 20_000_000 = 1.7 * 10^15. Seems about right (though this is using LeCun’s optical nerve figure -- divide by 10 if using the other bandwidth claims). Though note that the actual information content is probably OOMs lower.
# Grounding / Embodiment

## Richness of Interaction with the Real World

> Lex Fridman (00:05:57) Is it possible that language alone already has enough wisdom and knowledge in there to be able to, from that language, construct a world model and understanding of the world, an understanding of the physical world that you’re saying LLMs lack?

> Yann LeCun (00:06:56) So it’s a big debate among philosophers and also cognitive scientists, like whether intelligence needs to be grounded in reality. I’m clearly in the camp that yes, intelligence cannot appear without some grounding in some reality. It doesn’t need to be physical reality. It could be simulated, but the environment is just much richer than what you can express in language. Language is a very approximate representation [of] percepts and/or mental models. I mean, there’s a lot of tasks that we accomplish where we manipulate a mental model of the situation at hand, and that has nothing to do with language. Everything that’s physical, mechanical, whatever, when we build something, when we accomplish a task, model [the] task of grabbing something, et cetera, we plan or action sequences, and we do this by essentially imagining the result of the outcome of a sequence of actions that we might imagine and that requires mental models that don’t have much to do with language, and I would argue most of our knowledge is derived from that interaction with the physical world.

Claim: philosophers are split on grounding. LeCun participated in “Debate: Do Language Models Need Sensory Grounding for Meaning and Understanding?”. Otherwise I’m not so familiar with this debate.

> Lex Fridman (00:11:51) So you don’t think there’s something special to you about intuitive physics, about sort of common sense reasoning about the physical space, about physical reality. That to you is a giant leap that LLMs are just not able to do?

## Language / Video and Bandwidth

> Lex Fridman (00:17:44) I think the fundamental question is can you build a really complete world model, not complete, but one that has a deep understanding of the world?

# Hierarchical Planning

> Lex Fridman (00:44:20) So yes, for a model predictive control, but you also often talk about hierarchical planning. Can hierarchical planning emerge from this somehow?

# Skepticism of Autoregressive LLMs

> Lex Fridman (00:50:40) I would love to sort of linger on your skepticism around autoregressive LLMs. So one way I would like to test that skepticism is everything you say makes a lot of sense, but if I apply everything you said today and in general to I don’t know, 10 years ago, maybe a little bit less, no, let’s say three years ago, I wouldn’t be able to predict the success of LLMs. So does it make sense to you that autoregressive LLMs are able to be so damn good?

# RL(HF)

> Lex Fridman (01:29:38) The last recommendation is that we abandon RL in favor of model predictive control, as you were talking about, and only use RL when planning doesn’t yield the predicted outcome, and we use RL in that case to adjust the world model or the critic.

# Bias / Open Source

> Yann LeCun (01:36:23) Is it possible to produce an AI system that is not biased? And the answer is, absolutely not. And it’s not because of technological challenges, although they are technological challenges to that, it’s because bias is in the eye of the beholder. Different people may have different ideas about what constitutes bias for a lot of things, there are facts that are indisputable, but there are a lot of opinions or things that can be expressed in different ways. And so you cannot have an unbiased system, that’s just an impossibility.

# Business and Open Source

> Lex Fridman (01:49:59) Marc Andreessen just tweeted[3] today. Let me do a TL;DR. The conclusion is only startups and open source can avoid the issue that he’s highlighting with big tech. He’s asking, “Can Big Tech actually field generative AI products?”

> Yann LeCun (01:51:45) Mark is right about a number of things that he lists that indeed scare large companies. Certainly, congressional investigations is one of them, legal liability, making things that get people to hurt themselves or hurt others. Big companies are really careful about not producing things of this type because they don’t want to hurt anyone, first of all, and then second, they want to preserve their business. So it’s essentially impossible for systems like this that can inevitably formulate political opinions, and opinions about various things that may be political or not, but that people may disagree about, about moral issues and questions about religion and things like that or cultural issues that people from different communities would disagree with in the first place. So there’s only a relatively small number of things that people will agree on are basic principles, but beyond that, if you want those systems to be useful, they will necessarily have to offend a number of people, inevitably.

# Safety of (Current) LLMs

> Lex Fridman (01:55:13) But still even with the objectives of how to build a bioweapon, for example, I think something you’ve commented on, or at least there’s a paper where a collection of researchers is trying to understand the social impacts of these LLMs. I guess one threshold that’s nice is, does the LLM make it any easier than a search would, like a Google search would?

I kind of wish Lex had pushed more on why he thinks this will continue in the future.

# LLaMAs

> Lex Fridman (01:57:51) Just to linger on LLaMA, Marc announced that LLaMA 3 is coming out eventually. I don’t think there’s a release date, but what are you most excited about? First of all, LLaMA 2 that’s already out there and maybe the future a LLaMA 3, 4, 5, 6, 10, just the future of open source under Meta?

# GPUs vs the Human Brain

> Yann LeCun (02:02:32) We’re still far in terms of compute power from what we would need to match the compute power of the human brain. This may occur in the next couple of decades, but we’re still some ways away. Certainly, in terms of power efficiency, we’re really far, so there’s a lot of progress to make in hardware. Right now, a lot of the progress is, there’s a bit coming from silicon technology, but a lot of it coming from architectural innovation and quite a bit coming from more efficient ways of implementing the architectures that have become popular, basically combination of transformers and convnets, and so there’s still some ways to go until we are going to saturate. We’re going to have to come up with new principles, new fabrication technology, new basic components perhaps based on different principles and classical digital.

Claims:

- A human brain uses 25 watts. I found references in the 15-25 watt range.
- A brain is 10^5 - 10^6 times more powerful than a GPU. Joe Carlsmith covers this in great detail. He says "Overall, I think it more likely than not that 1e15 FLOP/s is enough to perform tasks as well as the human brain (given the right software, which may be very hard to create). And I think it unlikely (<10%) that more than 1e21 FLOP/s is required.". An H100 running FP8 calculations can do ~4e15 FLOPs. So, it's possible that one GPU is enough and at most 250,000. LeCun's claim is roughly an upper bound, but there's a lot of uncertainty.
# What does AGI / AMI Look Like?

> Lex Fridman (02:04:21) You often say that AGI is not coming soon, meaning not this year, not the next few years, potentially farther away. What’s your basic intuition behind that?

I'd love more detail on LeCun's reasons for ruling out recursive self-improvement / hard takeoff[4].

# AI Doomers

> Lex Fridman (02:08:48) So you push back against what are called AI doomers a lot. Can you explain their perspective and why you think they’re wrong?

I think this analogy breaks down in several ways and I wish Lex had pushed back a bit.

# What Does a World with AGI Look Like (Especially Re Safety)?

> Lex Fridman (02:15:16) So let’s imagine an AI system that’s able to be incredibly convincing and can convince you of anything. I can at least imagine such a system, and I can see such a system be weapon-like because it can control people’s minds. We’re pretty gullible. We want to believe a thing, and you can have an AI system that controls it and you could see governments using that as a weapon. So do you think if you imagine such a system, there’s any parallel to something like nuclear weapons?

# Big Companies and AI

> Lex Fridman (02:24:38) So let me ask you on your, like I said, you do get a little bit flavorful on the internet. Joscha Bach tweeted something that you LOL’d at in reference to HAL 9,000. Quote, “I appreciate your argument and I fully understand your frustration, but whether the pod bay doors should be opened or closed is a complex and nuanced issue.” So you’re at the head of Meta AI. This is something that really worries me, that our AI overlords will speak down to us with corporate speak of this nature, and you resist that with your way of being. Is this something you can just comment on, working at a big company, how you can avoid the over fearing, I suppose, through caution create harm?

# Hope for the Future of Humanity

> Lex Fridman (02:37:53) What hope do you have for the future of humanity? We’re talking about so many exciting technologies, so many exciting possibilities. What gives you hope when you look out over the next 10, 20, 50, a hundred years? If you look at social media, there’s wars going on, there’s division, there’s hatred, all this kind of stuff that’s also part of humanity. But amidst all that, what gives you hope?

# Summary

According to me, LeCun’s main (cruxy) differences from the median LW commenter:

- LeCun believes that autoregressive LLMs will not take us all the way to superintelligence.
- Downstream of that belief, he has longer timelines.
- LeCun sees embodiment as more important.
- LeCun believes in open development because he prioritizes different risks, seeing AI controlled by a small number of companies as the outcome to work against rather than a malevolent superintelligence.
Once you factor in the belief that transformers won't get us to superintelligence, LeCun's other views start to make more sense. Overall I came away with more uncertainty about the path to transformative AI (If you didn't, what do you know that LeCun doesn't?).

1. ^
1. ^
1. ^
1. ^
1. ^

