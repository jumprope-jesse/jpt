---
type: link
source: notion
url: https://chelseatroy.com/2018/04/05/how-do-we-make-remote-meetings-not-suck/
notion_type: Tech Deep Dive
tags: ['Running']
created: 2024-07-14T04:06:00.000Z
---

# How do we make remote meetings not suck? – Chelsea Troy

## AI Summary (from Notion)
- Main Topic: The challenges of achieving diversity and inclusion in tech organizations, focusing on the recruitment process.

- Key Points:
- Diversity is Not Just a Pipeline Issue: The lack of diversity is primarily an inclusion problem rather than a mere lack of applicants.
- Screening Process: Candidates from underrepresented groups actively screen potential employers based on the diversity of leadership. If they do not see diverse representation in senior roles, they are less likely to apply.
- Junior vs. Senior Roles: While hiring junior staff from underrepresented backgrounds is important, it does not address the leadership gap that affects recruiting senior candidates.

- Recruitment Strategies:
- Direct Outreach: Companies should directly contact potential candidates who meet their diversity goals, highlighting specific reasons for their interest.
- Preparation for Questions: Candidates will ask tough questions about company culture and diversity practices, and companies should be ready with honest, straightforward answers.
- Compensation: Offering competitive salaries and benefits is critical to attract diverse candidates, avoiding lowball offers that could deter applicants.

- Takeaways:
- Culture Shift Needed: Building a diverse workforce requires a cultural shift within organizations, ensuring that leadership is committed to and actively promoting diversity.
- Focus on Leadership: Prioritizing diversity in leadership positions is essential for fostering an inclusive environment that attracts diverse talent.
- Common Misconceptions: There is a misconception that discussing diversity in hiring equates to discrimination against white candidates, which undermines the need for diverse representation.

- Interesting Facts:
- False Positive Rates: When conducting multiple comparisons in statistical tests, the likelihood of false positives increases, emphasizing the need for careful analysis in hiring practices.
- Benjamini-Hochberg Procedure: This statistical method helps control the false discovery rate when making multiple comparisons in research, applicable to assessing diversity hiring outcomes.

- Conclusion: To effectively attract diverse talent, companies must address their internal cultures, improve leadership diversity, and engage directly with potential candidates while being prepared to answer their questions honestly and transparently.

## Content (from Notion)

Chelsea

Reading Time:  20 minutes

First we discussed the fact that technical issues cause remote meetings to suck.

Then, we discussed how even when there are no technical issues, remote meetings still suck.

They suck because they exacerbate a problem called the caucus problem. This is a problem even in meetings that are not remote, but remoteness makes the problem ubiquitous instead of isolated to some attendees.

One thing we did not do at the end of the last post is discuss how to fix the caucus problem. Instead, we took some time to reflect and absorb.

Now we’re going to talk about how to fix the caucus problem.

But first, we’re going to talk about how not to fix the caucus problem.

### One-Remote-All-Remote Does Not Solve the Problem

Remote meetings have sucked since long before I wrote about why remote meetings suck. Several companies with relatively successful remote programs have already taken a pass at finding a solution.

For example, Trello and Litmus recommend a one-remote all-remote meeting policy to make remote meetings suck less. Justine Jordan at Litmus explains: “Unless every person is in the same room, all meetings are held over video conference,” such that everyone dials in with headphones and mic to a remote meeting service like Google hangouts, Zoom, or appear.in.

This is a good step to ensure that the remote frustration exacerbation level is the same for everyone: since everyone gets the same lag, the same audio, and the same proximity to others, everyone is more considerate of remote attendees because everyone is a remote attendee. This additional consideration makes an all-remote meeting suck less than a meeting where you’re the only remote pipe-in.

But it doesn’t solve the problem because it treats remote meetings as the problem.

Remote meetings are not the problem. The caucus is the problem.

Remoteness just exacerbates the caucus problem so that even people with high caucus scores experience it.

Suppose we had the perfect one-remote all-remote setup—think Jedi Council from the prequels. Such a setup would elevate remote caucus meetings to the level of engagement in colocated caucus meetings—which is a shame, because now we’re back to the caucus score determining who gets to contribute. That excludes potential contributors.

As we have established, excluding potential contributors is a major problem.

This is a Jedi Council meeting in the Star Wars Universe. The ghosty characters are remote attendees.

### Limiting/Eliminating Remote Meetings Does Not Solve the Problem

Jason Fried of Basecamp offers another solution: we expect less of our remote meetings (presumably to treat them as coordination meetings and whatnot), then very occasionally meet in person to do ideation. He argues that, since companies can only execute on a few ideas at a time, we don’t need to do ideation meetings often, so we can save ideation for a rare, in-person meeting.

I don’t buy this argument. Set aside the question of how often you need new ideas. This argument again treats remote meetings as the problem; it speculates that ideation happens best in colocated meetings.

Remote meetings are not the problem. The caucus is the problem.

As we’ve established, caucus meetings are not good for getting ideas from your whole team. They’re good for getting ideas from the loudest and least inhibited members of your team, who I guarantee you, remote or not, will find a way to share their idea anyway.

### Telling People Not to Interrupt Does Not Solve the Problem

Let’s revisit our example of the caucus problem from the last post:

Latifah spends a few minutes presenting her perspective on something in a meeting. Her points are well-considered, and it’s clear that she has spent time considering this.

When Latifah finishes, Alessandra asks a poignant question the perspective Latifah just shared. Latifah (caucus score 6) has thought long and hard about that question, so she takes a breath and pauses for a moment to compose, in her head, a coherent explanation.

Just at that moment, Todd (caucus score 17) pipes up with his half-baked ramblings on the question. Latifah loses her chance to answer the question she was clearly well-positioned to answer, and the room spends 4 minutes listening to Todd umm and uhh his way to his opinion.*

What if Latifah takes Todd aside after the meeting and talks to him about what happened? She can express how it made her feel, he’ll feel bad, and maybe he won’t do it anymore. Maybe even Latifah’s manager joins her in the conversation to convey the gravity of the situation and address it on her behalf.

Sounds great, right? This is the Sesame Street solution that we were raised to favor: just talk it out! Unfortunately, this approach is not gonna solve your problem because it’s skipping a systemic problem and addressing an individual. By reprimanding Todd for interrupting, we’re cutting one flower off of our weed. But the root of the weed—the caucus—is still in the ground.

We’re treating Todd as the problem.

Todd is not the problem. Say it with me…

The caucus is the problem.

As a businessperson of some stripe I suspect you have read (or at least heard of) the canonical academic discourse about organizational incentive design: On the Folly of Rewarding A While Hoping for B (Steven Kerr, Academy of Management Journal, December 1975). In case you need a refresher, the crux of Kerr’s argument goes like this: If you’re rewarding one behavior while hoping for a mutually exclusive behavior, you are going to be disappointed because people will do the behavior you reward—not the behavior you say you want or think you want.

Caucuses give the speaking floor to people who take it. The meeting rewards ‘jumping in’. When you reprimand Todd for failing to ‘jump in’ perfectly, he’ll consider his decision on when to ‘jump in’ in the future. Sweet. So you’ll hear less from Todd the individual. Maybe that’s good, maybe that’s not.

But even setting that question aside, Todd is not your only Todd. There are other people in your organization—all people who, on the balance, want to contribute. The opportunity to contribute is granted to those who just start talking. Which means, at some point, this is going to happen again.

Are you going to talk to that person individually? Hmmm, maybe you can get out in front of it and talk to everyone about interrupting. So now you’re reprimanding people for interrupting when they haven’t done it yet (to their knowledge)—which can confuse them and make them feel micromanaged. And then what about when a new person joins the team, who didn’t hear your speech about interrupting? Someday in a caucus, they’re gonna interrupt someone. The interruptions will recur because we haven’t addressed the systemic problem: we’ve addressed an individual symptom. What we’re doing is asking people for a different behavior than the one we are rewarding. So, as Steven Kerr would say, we’re going to get A instead of B.

### How do we solve the problem?

To solve the problem, we have to address the incentive structure that rewards interrupting, penalizes listening, and provides no feedback from excluded team members…that is, we need to address the caucus.

The effective routes, in my experience, start with alternative incentive structures that don’t reward taking the floor without asking and instead reward asking permission, uplifting the contributions of others, and keeping remarks concise.

You’ll notice that our topic here has diverged from remoteness, specifically. That’s because better meetings benefit us whether we have all remote attendees, some remote attendees, or no remote attendees.

### We need an alternative to the caucus.

Instead of everyone talking whenever for however long, I recommend that you consider appointing a moderator for each of your meeting discussions. A skilled moderator can address the issues introduced by the caucus style and made universal by remote participation.

Allow me to situate the moderator role for you on the spectrum of roles that an individual can play in stewarding a conversation.

On one end of the spectrum, you have a chair. In meetings that use Robert’s Rules of Order (like United Nations Committees, Parliament, Congressional Committees, or Toastmasters Meetings), the chair usually refers to the person enforcing those rules. Chairs do not make the rules (though sometimes they modify them a little to suit the group). Chairs also do not involve themselves at all in the topic of discussion; instead, they make sure we go down the speaker list, they time each speaker, they ask how each speaker wishes to yield (spend) their extra time (to questions, to the chair, et cetera), and they process points of order or points of personal privilege (complaints and requests). They do not opine on the topic at hand.

On the other end, you have facilitators. Facilitation has a strong foothold in activist circles as a way to stoke discussion in groups of 5-20 people. I have seen it used in a couple of different anti-racist groups, and I have also seen it in a healthcare reform group. Unlike chairs, facilitators get to make up a lot of their own rules about who gets to talk, for how long, when to make exceptions, et cetera. Lots of facilitation guides and trainings also encourage facilitators to stoke discussion by involving themselves in said discussions. So you’ll see facilitators fact-checking what other contributors have said, editorializing at will on other people’s thoughts, mediating conflict, or guiding the direction of the conversation with leading questions. I’m not recommending that you do much of this for two reasons: first of all, if someone feels committed to a particular outcome for a conversation, they may rule on speaking rights accordingly. Second, it’s very easy for a facilitator to slide into centering themselves and their views in the discussion, instead of stewarding the discussion for the benefit of other contributors.

In between the chair role and the facilitator role, there’s a moderator: someone who combines the informal style of a facilitator with the chair-like behavior of not getting involved in the topic of discussion itself. This person has one responsibility during a meeting.

Moderator’s only responsibility: give people the opportunity to listen by safeguarding their opportunities to speak.

How does a moderator do this? They employ a number of techniques.

1. They make sure that everyone has the chance to see the agenda prior to the meeting so folks can take a look and prepare what they have to say.
1. They make sure everyone has access to the agenda during the meeting, so there are no surprises as to where (in general) the conversation goes.
1. They ask, before or during the meeting, who would like to talk about a topic. They make a list of these people. Then the moderator makes sure that these people get to speak, so they can stop worrying about jumping in and focus instead on listening to their peers.
1. If someone else realizes they want to speak, the moderator adds them to the list until discussion time runs out on this agenda item. The moderator can collect names via hand-raising or eye contact in a colocated meeting. In a part-remote or all-remote meeting, it’s most inclusive for a moderator to collect names with a messaging or chat app.
1. They stop people who interrupt the speaker and make sure the next person on the list is the next person to get time to speak.
1. Importantly, if the same four people are doing most of the talking, the moderator prioritizes and actively solicits contributions from the rest of the group. ‘This side of the room has not said much. Would anyone over here like to comment?’ Or ‘Maria, I know you worked on this. Do you have any thoughts on it?
1. If your company wants , you can try giving each speaker a time limit and, if they don’t use all their time, letting them accept questions with their remaining time if they want to. In this case the moderator keeps time for the speakers and, in the case of questions, reminds folks that questions are one sentence comprising a question: anything that’s ‘really more of a comment’ doesn’t belong here.
1. If there’s more to be said after time runs out on an agenda item, the moderator chooses to extend the time or recommend a separate meeting at another time on only this topic. If it’s a large meeting with limited time, the ‘separate meeting’ option ends up being the more prudent choice in my experience.
In a moderated conversation, folks get the opportunity to contribute by asking the moderator for a spot instead of participating in a speed-based elimination contest that starts over every time another person’s monologue is coming to an end. Instead of the floor (speaking rights) being taken, the floor is instead given (the official term is ‘yielded’).

### But Chelsea, we don’t want to moderate our meetings—that’s so formal!

So the thing is, human social relationships demand structure. We do not float around each other in a chaotic mass like amoebas in agar. In the absence of structural rules, we follow emergent social rules. Look at this entire article about how to walk on a sidewalk correctly. There’s a WikiHow page on how to ride an elevator. And you know what? Just because these rules aren’t “formal” doesn’t mean that people won’t think you’re just as much of a nincompoop for disobeying them. Go to New York City and walk down the left side of the sidewalk for ten blocks—I dare you.

These rules emerge in society, and they also emerge in your meetings. The unwritten caucus rule is an example of that. The rules that emerge in a caucus give more influence to the members of a group who already have the most influence. So if there are going to be rules anyway, there might as well be rules that help achieve the goal of the meeting—which is to gather valuable contributions from any/all meeting attendees (if the goal is instead to deliver info to the attendees, your meeting should be an email).

I suspect what you’re worried about is having meetings that feel too restrictive and too suit-and-tie. I have some good news:

1. Moderation will feel more restrictive to those with high caucus scores, but it will feel liberating to those with lower caucus scores. This tradeoff is worth the additional perspective you will add to decision-making.
1. You can have a moderated meeting that still feels modern and fun and looks nothing like a congress committee or Model UN.
In fact, you might have already seen some moderated or partly-moderated meetings in action without even realizing it.

### Some meetings already have a moderator.

Have you ever attended an iteration planning meeting? This is where a software development team sits down at the beginning of an iteration and plans the work they want to get done. The product manager has already organized a backlog of work to be completed. The team starts at the highest priority item in the backlog: the product manager explains it, developers ask questions, and they estimate its complexity. When all this is done, the product manager moves on to the next item down, and so on until the team has discussed enough work to fill the iteration.

This is a moderated meeting. The moderator in this case is usually the product manager. They command the floor by virtue of the fact that engineers are asking questions of them—not each other. If the questions specific to technical implementations such that developers are talking to each other, we’re now at a deeper level than “how complex is this”—which means this conversation is not for this meeting. So the product manager pulls it back in. “Are we ready to estimate?” The product manager collects everyone’s estimation and moves on. It’s clear who is meant to speak and when. We don’t move on until everyone who wants to talk has talked. The product manager is the one answering the questions, so there are no loudmouth developers overtaking the meeting. There’s a well-understood procedure for how the meeting goes, folks follow it, and everyone gets what they need to a larger degree than we see in an unmoderated caucus. These meetings, by the way, are pretty seamless to attend remotely.

Have you ever attended an iteration retrospective? This is where a software development team sits down at the end of an iteration and talks about how they can improve the next iteration. To do this, they go to a whiteboard or a shared document and they make four columns: Stuff that made me happy, stuff that made me confused or worried, stuff that made me sad or angry, and action items. Everyone gets to write down whatever they want in each of the first three columns about their experience in this past iteration. People can put checkmarks or plus signs next to others’ comments with which they agree.

Now here’s where this becomes a moderated meeting: when everyone finishes putting their items, a moderator—usually the newest team member, in my experience, but that’s arbitrary—goes through action items from the last iteration and asks about their status. Once that is done, the moderator moves on to this week: the person chooses items from the first three lists in whatever order they like. The person who wrote that item talks about the item, the team has a brief conversation as needed, an action item is added if appropriate, and the item is crossed off. So on until all the items are crossed off, at which point the moderator assigns the action items. There’s a well-understood procedure for how the meeting goes, folks follow it, and everyone gets what they need to a larger degree than we see in an unmoderated caucus. These meetings are also pretty seamless to attend remotely.

Have you been in meetings that always followed the same structure? Have you been in meetings where one person oversaw who talked and when? Did you feel your brilliance squashed like a bug beneath the authoritarian dogma of the meeting? My guess is no, for the most part. In such a meeting, attendees no longer have to compete for their opportunities to contribute. This is not repressive. It’s relieving.

### The more moderation people see, the less moderation they need.

I have noticed an interesting property of well-moderated meetings: by the end, the moderator doesn’t have to do much. Let’s take, as an example, a meeting on ethics in data science that I moderated and facilitated with some colleagues at a local conference in January.

My instincts are to avoid interjection, but I am a fairly firm moderator. If someone cuts in front of someone else to speak, I will calmly stop them and hand the floor back to the person who was previously speaking. Also, once I get a sense of who is doing the most talking, I’ll explicitly solicit the voices of those parts of the room that have so far remained silent, or I’ll use eye contact to signal to individuals and a subtle hand motion to inquire as to whether they would like to speak.

So I started off the meeting modeling that behavior. As is customary, toward the beginning of the meeting the higher caucus scores in the room made themselves known. They quickly and confidently signaled their desire to contribute on the majority of the questions I asked (question-asking is facilitating, not moderating. It serves a different goal than moderation. If you want we can talk about when and how to facilitate in another post). There were about four of them in a room of about eighteen. I gave them the floor, but once I had identified the pattern, I started to solicit input from the corners of the room that had so far not spoken. If several people wanted to speak at the same time, I explained that ‘John, you’re going to go, then Sarah, then Larry.’ If Larry or someone else started speaking after John, I would stop them and turn the floor over to Sarah.

Over the course of the meeting, people’s behavior changed—the group began to self-moderate. Speakers began explicitly turning the floor over to the next person in line. For example John would say ‘that’s what I think—I know Sarah has a great perspective on this’ (knowing Sarah’s turn was next). Or ‘I know Kai has an insight to share on this and Kai hasn’t spoken yet, so I’d love to hear from them.’ At one point toward the end of the meeting, my colleague Kat paused for a moment while considering her answer to a question someone had asked her. Someone else (let’s call him Bobbi) started to speak into the silence, interrupting Kat. I didn’t stop Bobbi—because as I opened my mouth to do so, a chorus of other people stopped Bobbi instead. Then all heads turned back to Kat, who smiled a little and finished her thought. After the meeting Bobbi apologized to Kat, but no one was upset with him. He had made a mistake in the heat of the moment, but the group had collectively limited its impact on the conversation, so no harm done. And I, as the moderator, didn’t have to say a word.

### Conclusion

Remote meetings suck because they exacerbate a problem called the caucus problem. This is a problem even in meetings that are not remote, but remoteness makes the problem ubiquitous instead of isolated to some attendees. This is why one-remote-all-remote solutions, or only doing ideation in colocated meetings, do not solve the problem: they target remoteness as the problem, and remoteness is not the problem. The caucus is the problem.

So how do we solve the caucus problem? Not with symptomatic solutions, like reprimanding people who interrupt. Instead, we have to get at the root of the problem: the way that caucuses reward interrupting, penalize listening, and provide no feedback from excluded team members.

Instead, we need an alternative incentive structure that doesn’t reward taking the floor without asking and instead rewards asking permission, uplifting the contributions of others, and keeping remarks concise. We can find such an alternative in the role of a moderator in our meetings.

A moderator doesn’t have to be too formal, and a moderator should not involve themselves in the discussion itself. Instead they have one job: give people the opportunity to listen by safeguarding their opportunities to speak.

A moderated meeting might sound ‘formal,’ but it involves no more rules than your current meeting: instead, it replaces implicit rules favoring people who already have influence with explicit rules that equalize that influence in the meeting. It also means less change than you think: some of your meetings may already include some moderation. In addition, a good model of moderation inspires your meeting attendees to self-moderate.

This does two things for you: first, it allows your moderator to moderate less and less as meetings progress. Second, it creates a self-perpetuating example for your colleagues to cultivate inclusive habits in their day-to-day interactions. When everyone shares the mindset of doing little things to acknowledge one another, solicit quiet voices, and protect each other’s opportunities to contribute, you have sown the seeds of the rare and valuable Inclusive Company Culture. May your garden grow.

Now, let’s talk about another common fear for work in a remote setting: what if I can’t tap my employee on the shoulder to get a quick answer to something?

### For more about remote meetings, check out:

Why are there always technical problems in remote meetings?

Why do remote meetings suck so much?

### For more about inclusion, you might enjoy:

The Seeds of a Great Company Culture

How your DudeBros are losing you money

Disrupting Management

### Share this:

Performance Speedometer

### When is it important to optimize on code performance?

It’s important to couch ‘performant’ in terms of what we’re optimizing.

We started this post by talking about what ‘performant’ means to us. In the default case, as programmers, when we say ‘performant’ we mean how fast the code runs. Why do we care how fast the code runs? Not just for hyuks (hopefully). Instead, the speed of the code translates to something we care about like page load time or job completion time.

Improving performance on pageload/job time might mean trading off performance on developer time. That should factor into our decisions about how to write code.

Two things affect developer time: clarity of code and robustness of code.

Clarity: Is it clear what the code is doing? How long does it take a dev to understand it? Would more performant code that minimizes page load or job completion time increase the amount of time developers spend understanding it?

Robustness: How likely is this code to break? If we trade it for code that takes less time to run, is that code more likely to break? Or is it more inscrutable such that a developer would not notice a flaw in it as easily? If not, and if we miss something, we lose a glut of developer time finding and fixing the bug later.

When we optimize on page load or job time, we’re talking milliseconds or, if it’s egregious, seconds. When we optimize on developer time, we’re often talking minutes or hours—or, not uncommonly, days. Because of this multiplicative difference, I’m optimizing on developer time until page load/job time matters.

When does page load/job time matter? It matters when humans have to wait on our code. The human eye registers about 60 frames per second. We cannot see things happening faster than that. And to do something about it? Even slower. The average human reaction time to a visual stimulus is about a quarter of a second. So if our page changes and our stuff on that page takes less than a quarter of a second to be ready, we’re beating the human. Faster than that, and a human will not notice.*

- Human reactions to audio stimuli are faster (somewhere between 0.15 and 0.20 seconds). But if your app is yelling/honking/dinging at people to get them to do things, you have bigger UX issues than performance.
When a load time or a job time becomes noticeable to testers, then we have reached the threshold where this metric matters. At that point, I address it even if the end result makes the code less dev-legible. But until then, our code isn’t failing to perform to the standard our users need, so I am usually unwilling to make a tradeoff on dev-legibility. Is there a situation in which I’ll make code more performant if it’s already performant enough? Yes: I’ll do it if more performant code is also more dev-legible.

### Would rewriting this code to reduce run time increase its developer time?

Here’s the code now:

```plain text
-    [self, chapter, section, unit, *ancestors].compact.flat_map(&:formula_ids)
```

Here are the queries that this code gives us:

```plain text
Section Load (5.4ms)  SELECT  "sections".* FROM "sections" WHERE "sections"."id" = $1 LIMIT 1  [["id", 542]]
Chapter Load (1.8ms)  SELECT  "chapters".* FROM "chapters" WHERE "chapters"."id" = $1 LIMIT 1  [["id", 163]]
Unit Load (5.0ms)  SELECT  "units".* FROM "units" INNER JOIN "chapters" ON "units"."id" = "chapters"."unit_id" INNER JOIN "sections" ON "chapters"."id" = "sections"."chapter_id" WHERE "sections"."id" = $1 LIMIT 1  [["id", 542]]
(6.1ms)  SELECT "formulas".id FROM "formulas" WHERE "formulas"."topic_id" = $1  [["topic_id", 268]]
(1.0ms)  SELECT "formulas".id FROM "formulas" WHERE "formulas"."chapter_id" = $1  [["chapter_id", 163]]
(0.9ms)  SELECT "formulas".id FROM "formulas" WHERE "formulas"."section_id" = $1  [["section_id", 542]]
(18.6ms)  SELECT "formulas".id FROM "formulas" WHERE "formulas"."unit_id" = $1  [["unit_id", 15]]

```

I count 7 queries.

Here’s the code optimized to the best of my immediate knowledge:

```plain text
Formula.where(topic_id: id).or(Formula.where(section_id: section.id)).or(Formula.where(chapter_id: chapter.id)).or(Formula.where(unit: unit.id)).pluck(:id)

```

Here’s the resulting queries:

```plain text
 Unit Load (0.5ms)  SELECT  "units".* FROM "units" INNER JOIN "chapter" ON "units"."id" = "chapter"."unit_id" INNER JOIN "sections" ON "chapter"."id" = "sections"."chapter_id" WHERE "sections"."id" = $1 LIMIT 1  [["id", 542]]
(0.8ms)  SELECT "formulas"."id" FROM "formulas" WHERE ((("formulas"."rule_id" = $1 OR "formulas"."section_id" = $2) OR "formulas"."chapter_id" = $3) OR "formulas"."unit_id" = $4)  [["topic_id", 268], ["section_id", 542], ["chapter_id", 163], ["unit_id", 15]]

```

2 queries, which would run faster.

I verified the two ActiveRecord Queries on the same topic, and I did get the same result.

So, what do we think of the code optimized on run time?

I find it clearer than what we had before.

How robust is it? Would the second code break where the first code wouldn’t? Doesn’t look like it right away to me (but someone check me on this if I’m missing something).

### Conclusion

In my humble view, optimizing this code on run time doesn’t hurt it on developer time, so it’s worth doing. But if the optimization had hurt the code on developer time, I would need verification from the QA team that they did, in fact, have to wait on this page to load before I would change it.

### If you like talking shop about app development, you might also like:

Reading Time:  41 minutes

Companies accrue ESG metrics that theoretically measure their performance on things like good governance, environmental stewardship, and fair employee treatment. I wanted to look for relationships between these metrics and company stock performance. Here’s the cumulative result so far!

# Millenials and Investment: an Ongoing Exploration

In case you haven’t heard, millenials are killing everything from diamonds to department stores to designer crap to grocery chains.

Why? Sure, the recession had an impact. But also, millenials pay more attention to ethics than many multinational corporations bargain for. They cite the blood diamond trade as a major reason to spring for non-traditional engagement rings. They opt for grocery providers that can tell them where their food is coming from and under what conditions it was produced. They’re ditching the fast fashion industry for higher-priced items purchased secondhand on sites like Poshmark and ThreadUp.

For millenials, investing and values go hand in hand

And as millenials reach the age where they might accrue some savings, it makes sense that they would care about where that is going, too. In addition to millenial attendance at the NoDAPL protests, we saw thousands of millenials divest from Western Union, Bank of America, and other banks that loaned money to the project. Maybe megacorps won’t change their tunes because a few thousand people stood in a field to get mowed down by water cannons, but they’re more likely to sit up and listen when those same people take their hard-earned doll hairs to another playhouse.

So we see that millenials are surveying their options to spend and save according to their values. What about investing? Any personal finance 101 that isn’t taught by a financial advisor will recommend a low cost index as the place to stick extra money so it can grow with the market. Most index funds, including the most recommended one (Vanguard), decide their investments via index-matching: matching their holdings to the S&P500 by market cap, with no other variables. Thing is, plenty of investors are expressing interest in taking ethical considerations into account. Some portfolios do this by blanket blocking investments in certain industries like tobacco or porn. Other more advanced optsions, like Betterment’s AutoSRI portfolio, use actual ESG data to determine where they invest the money. There isn’t (yet) a fully customizable option to allow folks to automatically invest their funds based on a checklist of their individual values. For a while, I’ve thought about building a toy version of what that might look like.

When I talk about the idea with friends and relatives, I get the following objection: ‘What about the returns?’ Touche. Nobody wants to lose out on their potential earnings. At first, I figured I’d build a tolerance into the system that allowed investors to say ‘These are my values, but please don’t invest in a way that will trail general market performance by more than x percent.’ The algorithm would then predict stock performance for each company, somehow blend that with ESG rating, and come up with a combined weight for divvying up investment money.

Before I build that, though, I need to test the assumption that high ESG ratings do correlate negatively with returns. If they don’t, there’s no need for the tolerance measure in the first place.

I’m not the first person to run correlations along these lines. Dorfleitner, Utz, and Wimmer published a paper on this just last year. Their analysis suggests that higher corporate social responsibility ratings increase returns over a long period of time (“long” being a 12 year period from 2002-2014). They even identify three specific areas that correlate with higher than average returns: emission and resource reduction, workforce, and society. So in my exploration, I’ll dig into some specific CSR breakdowns with the data I have on S&P 500 companies.

In [1]:

```plain text
import pandas as pd
import numpy as np

```

## Correlating KLD ESG Ratings to Stock Performance, 1990-2005

Let’s determine whether we notice any correlation between companies’ environmental, social, and governmental ratings and their stock performance.

### First, we pull in the ESG data.

These come from KLD and are now distributed by MSGI. I pulled them from an academic database. Don’t rerun this notebook because I didn’t push the actual data to Github, on account of it is large and on account of both data providers ask corporations to pay for the data. So I’m not going to undermine that.

In [2]:

```plain text
y91 = pd.read_excel('../stockproject/12231046.1990-1999.stats/1991 HistoricalSpreadsheet_STATS.xls')
y92 = pd.read_excel('../stockproject/12231046.1990-1999.stats/1992 HistoricalSpreadsheet_STATS.xls')
y93 = pd.read_excel('../stockproject/12231046.1990-1999.stats/1993 HistoricalSpreadsheet_STATS.xls')
y94 = pd.read_excel('../stockproject/12231046.1990-1999.stats/1994 HistoricalSpreadsheet_STATS.xls')
y95 = pd.read_excel('../stockproject/12231046.1990-1999.stats/1995 HistoricalSpreadsheet_STATS.xls')
y96 = pd.read_excel('../stockproject/12231046.1990-1999.stats/1996 HistoricalSpreadsheet_STATS.xls')
y97 = pd.read_excel('../stockproject/12231046.1990-1999.stats/1997 HistoricalSpreadsheet_STATS.xls')
y98 = pd.read_excel('../stockproject/12231046.1990-1999.stats/1998 HistoricalSpreadsheet_STATS.xls')
y99 = pd.read_excel('../stockproject/12231046.1990-1999.stats/1999 HistoricalSpreadsheet_STATS.xls')

nineties = [y91, y92, y93, y94, y95, y96, y97, y98, y99]

```

In [3]:

```plain text
y00 = pd.read_excel('../stockproject/12231046.2000-2009.stats/2000 HistoricalSpreadsheet_STATS.xls')
y01 = pd.read_excel('../stockproject/12231046.2000-2009.stats/2001 HistoricalSpreadsheet_STATS.xls')
y02 = pd.read_excel('../stockproject/12231046.2000-2009.stats/2002 HistoricalSpreadsheet_STATS.xls')
y03 = pd.read_excel('../stockproject/12231046.2000-2009.stats/2003 HistoricalSpreadsheet_STATS.xls')
y04 = pd.read_excel('../stockproject/12231046.2000-2009.stats/2004 HistoricalSpreadsheet_STATS.xls')
y05 = pd.read_excel('../stockproject/12231046.2000-2009.stats/2005 Historical Spreadsheet_STATS.xls') #wth KLD
y06 = pd.read_excel('../stockproject/12231046.2000-2009.stats/2006 Historical Spreadsheet_STATS.xls')
y07 = pd.read_excel('../stockproject/12231046.2000-2009.stats/2007 HistoricalSpreadsheet_STATS.xls')
y08 = pd.read_excel('../stockproject/12231046.2000-2009.stats/2008 HistoricalSpreadsheet_STATS.xls')
y09 = pd.read_excel('../stockproject/12231046.2000-2009.stats/2009 HistoricalSpreadsheet_STATS.xls')


two_thousands = [y00, y01, y02, y03, y04]

```

I wanted to run 1990-2009, but evidently starting in 2005 these spreadsheets no longer represent whether a company was in the S&P500 in the same way. That’s okay: we can do this for a fifteen-year period wrangle in more data later if we would like to see a longer period of time.

Now let’s pull out the companies that belong to the S&P 500. We’ll begin by examining their ESG scores on four metrics: employment policy strengths, employment policy concerns, environmental impact strengths, and environmental impact concerns. These companies get a score of zero (0) or one (1) each year on each of several sub-metrics. For example, employment policy strengths include specific scores for workplace safety, compensation, union management, et cetera.

We’re going to sum up all of the sub-metrics for each metric per company, and then we’re going to sum that company’s total scores in that metric across our fifteen year time span. We’ll end up with a company score of cumulative strengths and concerns in employment and environmental practices over the course of the fifteen years.

In [4]:

```plain text
twenty_years = nineties + two_thousands

def filter_500(df):
    return df[df['SP500'] == True]

sp500_90s = []
for data in twenty_years:
  sp500_90s.append(filter_500(data))

def aggregate_columns_for(category, dataframe):
    relevant_columns = [column_name for column_name in dataframe.columns.values if column_name.startswith(category)]
    return dataframe[relevant_columns].sum(axis=1)


aggregate_data = pd.concat(sp500_90s)
aggregate_data['emp_str_sum'] = aggregate_columns_for('EMP-str', aggregate_data)
aggregate_data['emp_con_sum'] = aggregate_columns_for('EMP-con', aggregate_data)
aggregate_data['env_str_sum'] = aggregate_columns_for('ENV-str', aggregate_data)
aggregate_data['env_con_sum'] = aggregate_columns_for('ENV-con', aggregate_data)

aggregate_data['alc_con_sum'] = aggregate_columns_for('ALC-con', aggregate_data)

aggregate_data['cgov_str_sum'] = aggregate_columns_for('CGOV-str', aggregate_data)
aggregate_data['cgov_con_sum'] = aggregate_columns_for('CGOV-con', aggregate_data)

aggregate_data['com_str_sum'] = aggregate_columns_for('COM-str', aggregate_data)
aggregate_data['com_con_sum'] = aggregate_columns_for('COM-con', aggregate_data)

aggregate_data['div_str_sum'] = aggregate_columns_for('DIV-str', aggregate_data)
aggregate_data['div_con_sum'] = aggregate_columns_for('DIV-con', aggregate_data)


aggregate_data.columns.values

```

Out[4]:

```plain text
array(['ALC-con-#', 'ALC-con-A', 'ALC-con-X', 'BMS', 'CGOV-con-#',
       'CGOV-con-B', 'CGOV-con-F', 'CGOV-con-G', 'CGOV-con-H',
       'CGOV-con-I', 'CGOV-con-X', 'CGOV-str-#', 'CGOV-str-A',
       'CGOV-str-C', 'CGOV-str-D', 'CGOV-str-E', 'CGOV-str-X', 'COM-con-#',
       'COM-con-A', 'COM-con-B', 'COM-con-D', 'COM-con-X', 'COM-str-#',
       'COM-str-A', 'COM-str-B', 'COM-str-C', 'COM-str-D', 'COM-str-F',
       'COM-str-G', 'COM-str-X', 'CUSIP', 'Com-con-A', 'CompanyName',
       'DIV-con-#', 'DIV-con-A', 'DIV-con-B', 'DIV-con-X', 'DIV-str-#',
       'DIV-str-A', 'DIV-str-B', 'DIV-str-C', 'DIV-str-D', 'DIV-str-E',
       'DIV-str-F', 'DIV-str-G', 'DIV-str-X', 'DS400', 'EMP-con-#',
       'EMP-con-A', 'EMP-con-B', 'EMP-con-C', 'EMP-con-D', 'EMP-con-X',
       'EMP-str-#', 'EMP-str-A', 'EMP-str-B', 'EMP-str-C', 'EMP-str-D',
       'EMP-str-F', 'EMP-str-G', 'EMP-str-X', 'ENV-con-#', 'ENV-con-A',
       'ENV-con-B', 'ENV-con-C', 'ENV-con-D', 'ENV-con-E', 'ENV-con-F',
       'ENV-con-X', 'ENV-str-#', 'ENV-str-A', 'ENV-str-B', 'ENV-str-C',
       'ENV-str-D', 'ENV-str-F', 'ENV-str-G', 'ENV-str-X', 'FIR-con-#',
       'FIR-con-A', 'GAM-con-#', 'GAM-con-A', 'GAM-con-X', 'HUM-con-#',
       'HUM-con-A', 'HUM-con-B', 'HUM-con-C', 'HUM-con-D', 'HUM-con-F',
       'HUM-con-G', 'HUM-con-X', 'HUM-str-#', 'HUM-str-A', 'HUM-str-D',
       'HUM-str-G', 'HUM-str-X', 'LCS', 'MIL-con-#', 'MIL-con-A',
       'MIL-con-B', 'MIL-con-C', 'MIL-con-X', 'NUC-con-#', 'NUC-con-A',
       'NUC-con-C', 'NUC-con-D', 'NUC-con-X', 'PRO-con-#', 'PRO-con-A',
       'PRO-con-D', 'PRO-con-E', 'PRO-con-X', 'PRO-str-#', 'PRO-str-A',
       'PRO-str-B', 'PRO-str-C', 'PRO-str-X', 'Russell1000', 'Russell2000',
       'SP500', 'TOB-con-#', 'TOB-con-A', 'TOB-con-X', 'Ticker',
       'emp_str_sum', 'emp_con_sum', 'env_str_sum', 'env_con_sum',
       'alc_con_sum', 'cgov_str_sum', 'cgov_con_sum', 'com_str_sum',
       'com_con_sum', 'div_str_sum', 'div_con_sum'], dtype=object)
```

OK, so here’s our data. Let’s take a look at this data and make sure we’re getting what we want: a sum of the ESG scores on a company-by-company basis.

In [5]:

```plain text
def sum_scores_for(dataframe, esg_marker):
    grouping = dataframe.groupby(['Ticker'])[esg_marker].sum()
    return pd.DataFrame({esg_marker : grouping}).reset_index()

esg_markers = [
    'emp_str_sum',
    'emp_con_sum',
    'env_str_sum',
    'env_con_sum',
    'alc_con_sum',
    'cgov_str_sum',
    'cgov_con_sum',
    'com_str_sum',
    'com_con_sum',
    'div_str_sum',
    'div_con_sum'
]

def aggregate_sums_for(esg_markers):
    esg_marker_data = sum_scores_for(aggregate_data, esg_markers[0])

    for esg_marker in esg_markers[1::]:
        esg_marker_data[esg_marker] = sum_scores_for(aggregate_data, esg_marker)[esg_marker]

    return esg_marker_data

esg_marker_data = aggregate_sums_for(esg_markers)

```

Ah, these sums look like what we would expect to see!

In [6]:

```plain text
esg_marker_data.sort_values(by=['emp_con_sum'], ascending=False).head()

```

Out[6]:

### Second, we pull in stock performance data.

This data contains stock returns by quarter for S&P500 companies dating back to 1979. We’ll pull the columns for the ’90s for now.

In [7]:

```plain text
price_data = pd.read_excel('../stockproject/Cleaned_Researcher_Dataset.xlsx')

```

In [8]:

```plain text
new_header = price_data.iloc[0] #grab the first row for the header
content = price_data[1:] #take the data less the header row
content.columns = new_header #set the header row as the df header
content.head()

tickers = content.iloc[:,0:2]
tickers.columns = list(new_header)[0:2]

dates = content.iloc[:,45:106]
dates.columns = list(new_header)[45:106]

result = pd.concat([tickers, dates], axis=1)
result.head()

```

Out[8]:

5 rows × 63 columns

### Third, we translate these stock prices into returns.

I want a metric that I can use to compare all the companies that a) belonged to the S&P 500 and b) earned some kind of KLD scores during the 1990-2005 period. Some companies only belong to the S&P500 for a subset of the years in question. We want a metric that will not penalize companies based on having spent less time in the S&P500, so a cumulative score won’t work for us. I decided to calculate quarterly returns based on the stock prices. This fairly compares each company’s stock performance during the period that an index-matching ETF would have held it, however long or short that was.

This is also nice because our mean function will only consider, for each company, those cells that have a number. So we don’t have to do as much data skullduggery to get the equation functions to spit out something meaningful.

In [9]:

```plain text
def quarter_return(start, end):
    if start == 0 or end == 0:
        return 0
    return end / start

#WARNING: This has to go column by column because the sequence in time matters.
#Such an iterative operation takes longer than async-per-column pandas operations.
#Expect this block of code to take several seconds to run.
raw_stock_prices = result
returns_df = raw_stock_prices[['Ticker', 'Company Name']]

#We cannot easily index the columsn by name
#because the column names are datetimes rather than strings,
#so we use column index instead.
for column_name in raw_stock_prices.iloc[:,2:]:
    loc = raw_stock_prices.columns.get_loc(column_name)
    this_column = raw_stock_prices.iloc[:,loc]
    next_col = loc + 1
    try:
        next_column = raw_stock_prices.iloc[:, next_col]

        temp_df = pd.concat([this_column, next_column], axis=1)
        temp_df.columns = ['a', 'b']

        returns_df['quarter_starting_' + column_name.strftime('%m/%d/%Y') + '_roi'] = (
        temp_df.apply(lambda row: quarter_return(row['a'], row['b']), axis=1))
    except:
        print('End of dataframe reached')

returns_df.head()

```

```plain text
//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:26: SettingWithCopyWarning:
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy

```

```plain text
End of dataframe reached

```

Out[9]:

5 rows × 62 columns

### Fourth, we combine the data into one dataframe.

We find all the companies for which we have both stock price data and ESG data, and we put the information together.

In [10]:

```plain text
tickers = list(esg_marker_data["Ticker"]) #get all the company tickers for which we have esg data
prices_for_esg_companies = returns_df[returns_df["Ticker"].isin(tickers)] #get the stock data from companies in that list
esg_marker_data.head()

```

Out[10]:

In [11]:

```plain text
relevant_esgs = esg_marker_data[esg_marker_data["Ticker"].isin(tickers)]
relevant_esgs = relevant_esgs.fillna(0.0) #no esg score to zero esg score
relevant_esgs['emp_str_sum'].unique()

```

Out[11]:

```plain text
array([16, 48,  2,  0, 22, 12, 18,  8,  6,  4, 28, 10, 20, 14, 36, 26, 30,
       24,  1, 42, 32, 11, 44, 52, 38, 13, 43, 54,  9, 72, 55,  7, 34, 40,
       60,  5, 25, 46, 58, 33, 50, 29,  3, 70, 62])
```

In [12]:

```plain text
all_data = pd.concat([relevant_esgs, prices_for_esg_companies.iloc[:,1:]], axis = 1) #put the esg and stock data in one dataframe
all_data = all_data[np.isfinite(all_data['emp_str_sum'])]

```

In [13]:

```plain text
all_data['emp_str_sum'].unique() #making sure the list of EMP-str-sum values is the same before and after concatenation to ensure that all the esg data made it over

```

Out[13]:

```plain text
array([ 16.,  48.,   2.,   0.,  22.,  12.,  18.,   8.,   6.,   4.,  28.,
        10.,  20.,  14.,  36.,  26.,  30.,  24.,   1.,  42.,  32.,  11.,
        44.,  52.,  38.,  13.,  43.,  54.,   9.,  72.,  55.,   7.,  34.,
        40.,  60.,   5.,  25.,  46.,  58.,  33.,  50.,  29.,   3.,  70.,
        62.])
```

### Fifth, we look at the data.

In [14]:

```plain text
import matplotlib.pyplot as plt
import random
import seaborn as sns
import math
from scipy.stats import t
import numpy as np

%matplotlib inline

```

Our research question, recall, is this: do companies with low aggregated ESG scores have better stock performance than companies with high aggregated ESG scores?

We have so far calculated the return on investment of every stock, every quarter. We’re not ready to use that data, though. If we use each return individually, we’ll end up comparing each example (that is, each company) multiple times. Multiple comparisons in statistics gives you more opportunities to end up with false positives because it gives you more cracks at hitting that 5% probability lottery (if p-0.05) of your results being a fluke.

Instead, we want a way to generalize over all of its quarterly returns for those quarters in which each stock had a return.

Easy! Take an average, right? Well, not quite.

First of all, an average skews toward big outliers. If most quarters showed an ROI of 1.01 and then one quarter has an ROI of 2, the average will skew much higher than 1.01.

Pursuant to that, smaller numbers of measurements skew toward the extremes. So if a company only has three returns, each individual return affects the average a lot. That means, if one of the three is super high, the whole average will be super high. Comparatively, if a company has 200 returns and one of them is an outlier, it won’t drag the average nearly as far. In datasets where examples possess a different number of measurements, the highest and lowest values for the target variable often come from samples with few measurements.

We are in a prime position to encounter that gotcha, because some of these companies are much more measured (many more quarterly returns) than others.

So instead, we’re going to do something that might look a little weird.

### Confidence Intervals

Instead of going off the average of our set of returns, we’ll take the confidence interval for it. A confidence interval attempts to take into account the fact that our data doesn’t quite match the real world: it samples the real world such that we can attempt to represent the real world in studies. So the “true average” of something might not look like the average of our measurements of it, and the fewer measurements we have the less sure we can be of the discrepancy. A confidence interval takes our average and says ‘based on this average, the true average lies between this number and this number with this probability.’

So, for our stock return measurements, we’ll get the confidence interval for each company. For companies with few measurements, that confidence interval will be wide. For companies with many measurements, it will be more narrow. We’ll then sample that pessimistically and compare the bottoms of all the companies’ confidence intervals.

It’s worth noting that this is going to give us a very low number for companies with few returns and without a high outlier. We’ll address that as well.

In [15]:

```plain text
# Calculate average roi
returns = all_data[all_data.columns.difference(['emp_str_sum', 'emp_con_sum', 'env_str_sum', 'env_con_sum', 'div_str_sum', 'Ticker','Company Name'])]
returns.head()

```

Out[15]:

5 rows × 66 columns

In [16]:

```plain text
def confidence_interval_for(samples=[], confidence=0.95):
    sample_size = len(samples)
    degrees_freedom = sample_size - 1
    outlier_tails = (1.0 - confidence) / 2.0
    t_distribution_number = -1 * t.ppf(outlier_tails, degrees_freedom)

    step_1 = np.std(samples)/math.sqrt(sample_size)
    step_2 = step_1 * t_distribution_number

    low_end = np.mean(samples) - step_2
    high_end = np.mean(samples) + step_2

    return low_end, high_end

```

In [17]:

```plain text
lower_conf_intervals = []
upper_conf_intervals = []

for (idx, row) in returns.iterrows():
    maybe_measurements = row.tolist()
    measurements = [float(x) for x in maybe_measurements if (math.isnan(float(x)) == False)]
    bottom, top = confidence_interval_for(measurements)
    lower_conf_intervals.append(bottom)
    upper_conf_intervals.append(top)

```

In [18]:

```plain text
all_data['avg_quarterly_roi'] = returns.mean(axis=1, skipna=True)
all_data['num_measurements'] = returns.count(axis=1)
all_data['lower_conf_interval'] = lower_conf_intervals
all_data['upper_conf_interval'] = upper_conf_intervals

all_data.head(10)

```

Out[18]:

10 rows × 77 columns

Before we go on, let’s talk statistical power.

The statistical power of a dataset describes the likelihood that you will be able to detect a meaningful difference with this data given that the difference is there. It’s an important precursor step to data analysis because it determines your ability to find the effect you’re looking for.

Calculating statistical power is a little complicated, and it’s generally a good idea to get someone who knows what they’re doing to help you. I’m not confident in my ability to evaluate statistical power calculation strategies yet, but I figured I’d start with the all-around method and used this online calculator.

I performed a one-tail statistical power test. For this we will need the following data:

In [19]:

```plain text
all_data['lower_conf_interval'].describe()

```

Out[19]:

```plain text
count    925.000000
mean      -0.221716
std        1.515036
min      -16.108048
25%       -0.688611
50%        0.228662
75%        0.799898
max        3.885452
Name: lower_conf_interval, dtype: float64
```

I took the mean of the bottom end of confidence intervals on stock returns—0.92. Suppose this were only the mean for companies with favorable ESG scores, and suppose that companies with unfavorable ESG scores had a higher quarterly return of 1 standard deviation higher—0.934. So 0.92 is my sample average and 0.934 is my test value.

In calculating your statistical power, you have two variables under your control: the number of examples in your data and your effect size. The more examples you have, the smaller effect size you can reliably detect. This means that, if you have already collected your data (as we have), to have a statistically powerful study, the effect size you’re seeking needs to be large enough that the number of examples you have could reliably detect it. The bigger the effect size, the less data you need to detect it. For small effect sizes, you may need thousands of examples. But if we had only ten stock examples (5 with a low ESG aggregate score, 5 with a high one) and the 5 with low ESG aggregate scores showed triple the returns of the high ones, that effect size is huge, so our tiny study could still reliably detect it. (to check this, I plugged it into the power calculator with an absurdly high standard deviation of 1. My power was 100%).

Anyway, back to our real data. The sample size is how many examples we have…

In [20]:

```plain text
all_data.shape

```

Out[20]:

```plain text
(925, 77)
```

…925.

Our standard deviation for sample is 0.14. so we plug that in.

I left the confidence level at 5% and ran the calculator.

My result: 91.9%

That’s the probability, if there is a 1.4 percentage point difference or more between stock returns for companies with favorable and unfavorable ESG scores, we have a 91.9% chance of detecting it. Statistical power expectations vary, but a rule of thumb (and standard practice at some more rigorous scientific journals) is to expect a statistical power of 80% or higher to take a study seriously.

OK, time to make some pictures! Let’s plot our ESG score sums against average roi and see if we notice any trends.

In [21]:

```plain text
f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(20, 10))
f.suptitle('ESG Sums Plotted Against Returns')
ax1.scatter(x=all_data['emp_str_sum'], y=all_data['lower_conf_interval'])
ax2.scatter(x=all_data['emp_con_sum'], y=all_data['lower_conf_interval'])
ax3.scatter(x=all_data['env_str_sum'], y=all_data['lower_conf_interval'])
ax4.scatter(x=all_data['env_con_sum'], y=all_data['lower_conf_interval'])
all_data.shape

```

Out[21]:

```plain text
(925, 77)
```

We have a couple of really low ones on those charts. Let’s look at them:

In [22]:

```plain text
all_data[['Company Name','lower_conf_interval', 'num_measurements']].sort('lower_conf_interval').head(10)

```

```plain text
//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)
  if __name__ == '__main__':

```

Out[22]:

We spoke a little bit earlier about the way our measuring strategy would especially penalize companies with only a few measurements. We’re seeing that here. Out of these bottom 10 scores for the lower end of stock return confidence intervals, only one has more than 7 data points. Compare this to the mean number of measurements:

In [23]:

```plain text
all_data['num_measurements'].mean()

```

Out[23]:

```plain text
25.932972972972973
```

But there’s another thing to note about this data: five of the values are negative. These scores are low, yes, but a negative value for a stock return does not make sense. The value of a stock can drop to zero, but it does not turn into debt to the company owed by the stockholders. The lowest a stock price can drop is to zero. To accurately represent that, we’ll clip these values at zero.

In [24]:

```plain text
all_data['lower_conf_interval'] = all_data['lower_conf_interval'].clip_lower(0)

```

Let’s check the means too to make sure we don’t have negative values in the means:

In [25]:

```plain text
all_data[['Company Name','avg_quarterly_roi', 'num_measurements']].sort('avg_quarterly_roi').head(10)

```

```plain text
//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)
  if __name__ == '__main__':

```

Out[25]:

Cool: we don’t. This means that none of the upper ends of the confidence intervals will have negative values either, as the upper end of the confidence interval is always higher than the mean on which it is based.

Let’s check out our plot again with those adjustments:

In [26]:

```plain text
f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(20, 10))
f.suptitle('ESG Sums Plotted Against Returns')
ax1.scatter(x=all_data['emp_str_sum'], y=all_data['lower_conf_interval'])
ax2.scatter(x=all_data['emp_con_sum'], y=all_data['lower_conf_interval'])
ax3.scatter(x=all_data['env_str_sum'], y=all_data['lower_conf_interval'])
ax4.scatter(x=all_data['env_con_sum'], y=all_data['lower_conf_interval'])

```

Out[26]:

Let’s look at the general trends of the returns. They look pretty flat to me: I don’t see an upward or downward trend in quarterly ROI based on any of the 4 ESG metrics.

Let’s dig a little deeper and see if the numbers themselves support that.

In [27]:

```plain text
f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(20, 10))
f.suptitle('Histograms of Aggregated ESG Scores')
ax1.hist(x=all_data['emp_str_sum'])
ax2.hist(x=all_data['emp_con_sum'])
ax3.hist(x=all_data['env_str_sum'])
ax4.hist(x=all_data['env_con_sum'])
all_data.shape

```

Out[27]:

```plain text
(925, 77)
```

The thing about a lot of our data is that it skews heavily toward low ESG sums. For example, there are a lot more companies with low employment policy strength scores than high ones. How do we account for this when we do our aggregations?

Well, we’ll look at the individual ESG scores first. But later we’ll bucket those ESG scores by fives so that we have small groups of scores, and then we’ll look at the confidence intervals around those (which will, of course, be wider for smaller groups).

Here’s a question though—why do buckets at all? Why not stick to a linear regressor?

Good question! A regressor can frequently be a preferable option to drawing arbitrary distinctions in continuous data. This isn’t exaclty continuous data as the ESG aggregations are all natural numbers, but that is not the reason for the buckets. The reason for the buckets is to have some kind of aggregate at each stage across our data examples, so our data is not so broken up by a single ESG score with no examples, or just one example. If we run a regressor, the bottom of the regressor represents a ton of data points, and as we go higher it represents fewer and fewer data points such that the expected value up there means very little.

I’d be very concerned to artificially divide this data into only two buckets such that points close to the arbitrary division get lumped in with very different means. But breaking it up by fives gives us several buckets such that each bucket of data points contains data points that are roughly similar to one another in their ESG scores.

In [28]:

```plain text
all_data['emp_str_buckets'] = all_data.apply(lambda row: int(row['emp_str_sum'] / 5), axis=1)
all_data['emp_con_buckets'] = all_data.apply(lambda row: int(row['emp_con_sum'] / 5), axis=1)
all_data['env_str_buckets'] = all_data.apply(lambda row: int(row['env_str_sum'] / 5), axis=1)
all_data['env_con_buckets'] = all_data.apply(lambda row: int(row['env_con_sum'] / 5), axis=1)

```

In [29]:

```plain text
def get_grouping_for(esg_metric, dataframe):
    return dataframe.groupby(esg_metric).agg({'lower_conf_interval': ['count','mean','std']}).reset_index()

emp_str_sum_group = get_grouping_for('emp_str_sum', all_data)
emp_con_sum_group = get_grouping_for('emp_con_sum', all_data)
env_str_sum_group = get_grouping_for('env_str_sum', all_data)
env_con_sum_group = get_grouping_for('env_con_sum', all_data)

f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(20, 10))
f.suptitle('ESG Sums Plotted Against Returns')
ax1.plot(emp_str_sum_group['emp_str_sum'], emp_str_sum_group['lower_conf_interval']['mean'])
ax2.plot(emp_con_sum_group['emp_con_sum'], emp_con_sum_group['lower_conf_interval']['mean'])
ax3.plot(env_str_sum_group['env_str_sum'], env_str_sum_group['lower_conf_interval']['mean'])
ax4.plot(env_con_sum_group['env_con_sum'], env_con_sum_group['lower_conf_interval']['mean'])

```

Out[29]:

```plain text
[]
```

Hmm. I’m not sure I see much of a trend here. Let’s see if any of these ESG sums correlate with their mean lower confidence interval return.

In [30]:

```plain text
all_data[['emp_str_sum', 'emp_con_sum', 'env_str_sum', 'env_con_sum', 'lower_conf_interval']].corr()

```

Out[30]:

We’re interested in the correlations between the esg sums on the left and the lower_conf_interval (last column). These are all very small correlations. A perfect direct correlation is 1. A perfect inverse correlation is -1. Zero means no correlation whatsoever. These correlation numbers arepretty darn close to zero.

We’ll have to dig deeper into the data.

### Sixth, we determine what our results could mean.

OK friends, it’s time to figure out what our data means, or rather, whether it means anything at all with respect to our question: do companies with higher str ESG scores and lower con ESG scores have lower stock returns than companies that perform more poorly on ESG metrics?

First, let’s calculate our confidence intervals for stock performance and determine if the stock performances for companies low ESG scores fall outside the confidence intervals for the stock performance of companies with high ESG scores.

In [31]:

```plain text
import math
from scipy.stats import t
import numpy as np

```

First, let’s make ourselves a method to get confidence intervals from summary statistics. That way, we can make interpretable groupings for all of our esg scores and get the confidence intervals on those scores based on how many companies had each esg score.

In [32]:

```plain text
def confidence_interval_for_collection(sample_size=[], standard_deviation=[], mean=[], confidence=0.95):
    degrees_freedom = [count - 1 for count in sample_size]
    outlier_tails = (1.0 - confidence) / 2.0
    confidence_collection = [outlier_tails for _ in sample_size]
    t_distribution_number = [-1 * t.ppf(tails, df) for tails, df in zip(confidence_collection, degrees_freedom)]

    step_1 = [std/math.sqrt(count) for std, count in zip(standard_deviation, sample_size)]
    step_2 = [step * t for step, t in zip(step_1, t_distribution_number)]

    low_end = [mean_num - step_num for mean_num, step_num in zip(mean, step_2)]
    high_end = [mean_num + step_num for mean_num, step_num in zip(mean, step_2)]

    return low_end, high_end

```

Now let’s make ourselves some convenience methods to wrhangle our data. One of them will create the grouping we talked about before, aggregating each ESG metric to tell us a) how many companies had each score on the ESG metric, b) the mean return in that group of companies, and c) the standard deviation on returns in that group of companies.

We’ll use that data to get a lower limit and an upper limit on a confidence interval around those returns based on how many measurements we have at each score.

Finally, we’ll make a method to plot the results of these groupings so we can eyeball our results.

In [33]:

```plain text
def prepare_confidence_data_for(esg_metric, grouping):
    labels=grouping[esg_metric]
    sample_size = grouping['lower_conf_interval']['count']
    standard_deviation = grouping['lower_conf_interval']['std']
    mean = mean = grouping['lower_conf_interval']['mean']

    low_end, high_end = confidence_interval_for_collection(sample_size, standard_deviation, mean)
    df_with_confidence_limits = pd.concat([labels, sample_size, standard_deviation, mean], axis=1)
    df_with_confidence_limits['.95 confidence level min'] = low_end
    df_with_confidence_limits['.95 confidence level max'] = high_end
    return df_with_confidence_limits

def plot_grouping(group_name, grouping, axes):
    mini = axes.plot(grouping['.95 confidence level max'], color='r', label="Max of 0.95 Confidence Interval")
    mean = axes.plot(grouping['mean'], color='g', label="Mean")
    maxi = axes.plot(grouping['.95 confidence level min'], color='b', label="Min of 0.95 Confidence Interval")

    legend = axes.legend(loc='upper right', shadow=True)

```

Let’s eyeball some charts!

In [34]:

```plain text
emp_str_confidence_interval_data = prepare_confidence_data_for('emp_str_sum', emp_str_sum_group)
emp_con_confidence_interval_data = prepare_confidence_data_for('emp_con_sum', emp_con_sum_group)
env_str_confidence_interval_data = prepare_confidence_data_for('env_str_sum', env_str_sum_group)
env_con_confidence_interval_data = prepare_confidence_data_for('env_con_sum', env_con_sum_group)

f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(20, 10))
f.suptitle('ESG Sums Plotted Against Returns')
plot_grouping('Employment Strength Sums', emp_str_confidence_interval_data, ax1)
plot_grouping('Employment Concern Sums', emp_con_confidence_interval_data, ax2)
plot_grouping('Environmental Strength Sums', env_str_confidence_interval_data, ax3)
plot_grouping('Emnvironmental Concern Sums', env_con_confidence_interval_data, ax4)

```

Worth noting: You can tell from the cart the rough relative number of data points that we had for a given ESG score. Where you see no red, blue, or green lines, we had no data points for that ESG score. Where there is a green line but no red or blue lines, we had only one data point for that ESG score. Where there are red and blue lines and they are far apart, we had a few data points, but a small enough number that the confidence interval is still very wide. Where the red and blue lines flank the green line very closely, we had a lot of data: the confidence interval for these is very close to the mean.

This is a little hard to read with the skips in it. Let’s bucket the scores and see how that looks:

In [35]:

```plain text
emp_str_bucket_group = get_grouping_for('emp_str_buckets', all_data)
emp_con_bucket_group = get_grouping_for('emp_con_buckets', all_data)
env_str_bucket_group = get_grouping_for('env_str_buckets', all_data)
env_con_bucket_group = get_grouping_for('env_con_buckets', all_data)

emp_str_bucket_data = prepare_confidence_data_for('emp_str_buckets', emp_str_bucket_group)
emp_con_bucket_data = prepare_confidence_data_for('emp_con_buckets', emp_con_bucket_group)
env_str_bucket_data = prepare_confidence_data_for('env_str_buckets', env_str_bucket_group)
env_con_bucket_data = prepare_confidence_data_for('env_con_buckets', env_con_bucket_group)

f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(20, 10))
f.suptitle('ESG Sums Plotted Against Returns')
plot_grouping('Bucketed Employment Strength Sums', emp_str_bucket_data, ax1)
plot_grouping('Bucketed Employment Concern Sums', emp_con_bucket_data, ax2)
plot_grouping('Bucketed Environmental Strength Sums', env_str_bucket_data, ax3)
plot_grouping('BucketedEmnvironmental Concern Sums', env_con_bucket_data, ax4)

```

Check out those widening confidence intervals as we get to the higher point ranges! You can practicelly see the number of exaxmples falling off.

So what am I looking for here? I’m looking to see if the highest point on the bottom of the confidence intervals on either side of the graph is higher than the top of the confidence intervals on the other side. This break in overlap between the confidence intervals would provide a leading indication of a strong and meaningful difference in stock returns for companies with high versus low scores in any of these ESG metrics.

I don’t see that happen in any of these charts. The confidence intervals overlap significantly all the way across. So we can’t be confident at all that stocks from the high ESG score groups will have higher or lower returns than the stocks from the low ESG score groups.

But wait! That doesn’t necessarily mean that there is no meaningful difference. The confidence intervals can overlap for two series of points whose data still has a meaningful difference. To determine whether it does, we need to perform additional significance testing—in this case, a t test.

How to do the independent samples t test

In [36]:

```plain text
def t_test_for(num_samples_1, standard_deviatcion_1, mean1, num_samples_2, standard_deviation_2, mean2, confidence=0.95, num_comparisons=1):
  alpha = (1 - confidence)/float(num_comparisons)
  total_degrees_freedom = num_samples_1 + num_samples_2 - 2

  t_distribution_number = -1 * t.ppf(alpha, total_degrees_freedom)

  degrees_freedom_1 = num_samples_1 - 1
  degrees_freedom_2 = num_samples_2 - 1
  sum_of_squares_1 = (standard_deviation_1 ** 2) * degrees_freedom_1
  sum_of_squares_2 = (standard_deviation_2 ** 2) * degrees_freedom_2

  combined_variance = (sum_of_squares_1 + sum_of_squares_2) / (degrees_freedom_1 + degrees_freedom_2)
  first_dividend_addend = combined_variance/float(num_samples_1)
  second_dividend_addend = combined_variance/float(num_samples_2)

  denominator = math.sqrt(first_dividend_addend + second_dividend_addend)
  numerator = mean1 - mean2
  t_value = float(numerator)/float(denominator)

  accept_null_hypothesis = abs(t_value) < abs(t_distribution_number) #results are not significant

  return accept_null_hypothesis, t_value

```

We have written a method that will tell us, firstly, whether to accept or reject the null hypothesis, which assumes no meaningful difference between the two sets of data we want to compare. I have named that output ‘accept_null_hypothesis’ because I don’t love the ubiquitous use of the confounding phrase ‘reject the null hypothesis’ in scientifi inquiry. It’s a double negative (reject the absence of meaningful difference), which adds an unnecessary additional piece of mental acrobatics to the (already frequently herculean) task of determining what, exactly, the scientists are trying to say in their conclusion paragraph.

We are going with accept the absence of meaningful difference as the variable name for two reasons. First of all, we remove the double negative this way. Second of all, accepting the null hypothesis is (or should be) the outcome of the vast majority of scientific inquiry. Scientists, collectively, test a whole bunch of stuff to see what has an effect. Most of the things tried, it turns out, don’t have that effect. So our accept_null_hypothesis value will usually be true. When it’s false, we should sit up and pay attention.

In [37]:

```plain text
mean1 = all_data.sort('emp_str_sum', ascending=False).iloc[0:20, :]['lower_conf_interval'].mean()
std1 = all_data.sort('emp_str_sum', ascending=False).iloc[0:20, :]['lower_conf_interval'].std()

mean2 = all_data.sort('emp_str_sum').iloc[0:20, :]['lower_conf_interval'].mean()
std2 = all_data.sort('emp_str_sum').iloc[0:20, :]['lower_conf_interval'].std()

n1 = 20
n2 = 20

accept_null_hypothesis, t_value = t_test_for(n1, std1, mean1, n2, std2, mean2)
print(accept_null_hypothesis)
print(t_value)

```

```plain text
True
0.9045209709738401

```

```plain text
//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)
  if __name__ == '__main__':
//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:2: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)
  from ipykernel import kernelapp as app
//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:4: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)
//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:5: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)

```

In [38]:

```plain text
mean1 = all_data.sort('emp_con_sum', ascending=False).iloc[0:20, :]['lower_conf_interval'].mean()
std1 = all_data.sort('emp_con_sum', ascending=False).iloc[0:20, :]['lower_conf_interval'].std()

mean2 = all_data.sort('emp_con_sum').iloc[0:20, :]['lower_conf_interval'].mean()
std2 = all_data.sort('emp_con_sum').iloc[0:20, :]['lower_conf_interval'].std()

n1 = 20
n2 = 20

accept_null_hypothesis, t_value = t_test_for(n1, std1, mean1, n2, std2, mean2)
print(accept_null_hypothesis)
print(t_value)

```

```plain text
True
0.8148459441345176

```

```plain text
//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)
  if __name__ == '__main__':
//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:2: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)
  from ipykernel import kernelapp as app
//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:4: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)
//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:5: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)

```

In [39]:

```plain text
mean1 = all_data.sort('env_str_sum', ascending=False).iloc[0:20, :]['lower_conf_interval'].mean()
std1 = all_data.sort('env_str_sum', ascending=False).iloc[0:20, :]['lower_conf_interval'].std()

mean2 = all_data.sort('env_str_sum').iloc[0:20, :]['lower_conf_interval'].mean()
std2 = all_data.sort('env_str_sum').iloc[0:20, :]['lower_conf_interval'].std()

n1 = 20
n2 = 20


accept_null_hypothesis, t_value = t_test_for(n1, std1, mean1, n2, std2, mean2)
print(accept_null_hypothesis)
print(t_value)

```

```plain text
True
0.5527330264232702

```

```plain text
//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)
  if __name__ == '__main__':
//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:2: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)
  from ipykernel import kernelapp as app
//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:4: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)
//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:5: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)

```

In [40]:

```plain text
mean1 = all_data.sort('env_con_sum', ascending=False).iloc[0:20, :]['lower_conf_interval'].mean()
std1 = all_data.sort('env_con_sum', ascending=False).iloc[0:20, :]['lower_conf_interval'].std()

mean2 = all_data.sort('env_con_sum').iloc[0:20, :]['lower_conf_interval'].mean()
std2 = all_data.sort('env_con_sum').iloc[0:20, :]['lower_conf_interval'].std()

n1 = 20
n2 = 20

accept_null_hypothesis, t_value = t_test_for(n1, std1, mean1, n2, std2, mean2)
print(accept_null_hypothesis)
print(t_value)

```

```plain text
True
1.637305984889984

```

```plain text
//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)
  if __name__ == '__main__':
//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:2: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)
  from ipykernel import kernelapp as app
//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:4: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)
//anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:5: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)

```

How about that! So far, accepting the null hypothesis everywhere. It appears that we cannot meaninfgully separate the stock performance of companies on either end of the spectrum for each of their employment policy ESG scores.

### Interim Conclusions

On this pass, we have taken a much more thorough look at our data with respect to how well it approximates the real world and how certain we are about that approximation.

This did not mean tacking on a test for statistical significance at the end: it meant rethinking our analysis from the very beginning, including what our target variable would be (previously extrapolated annual roi, now back to mean quarterly roi) and also how we would represent it (with the bottom end of the confidence interval instead of the mean itself).

It also meant checking our results more thoroughly along the way, identifying unrealistic values in the data, understanding why those values were what they were, and replacing them with values that better approximate reality.

Finally, it meant conducting a t test rather than eyeballing the results.

The result here provides an excellent example of the difference between how data might be interpreted or perceived versus what it really represents. When we use the bottom of the confidence intervals, we’re looking at a lot of losses in shareholder value. It’s important to note that this isn’t really what happened. It is, more accurately, a statistically derived worst-case scenario for a company that did everything that a given example company did, based on the metrics we have about how that company actually did. That is not useful for predicting what the stock returns for the example company will be in the future (fun fact: trying to predict stock returns in general is not that useful, as straight-up index matching outperforms the vast majority of shrewder, prediction-based investment strategies in the long term. I don’t miss our loss of predictive value because there wasn’t much demonstrated value there to begin with.)

At any rate, what it is useful for is accurately representing the fact that there is a lot we do not know and attempting to account for that in a comparison of stock returns relative to ESG ratings. We’re not trying to predict returns by company: we’re trying to determine if companies with favorable ESG scores, in aggregate, underperform on the stock market relative to companies with unfavorable ESG scores. For this, our data is useful.

And, as we’ve established, we don’t see in this analysis a meaningful difference in any of our cases. This bodes well for the application question—can I create a socially conscious investment portfolio without sacrificing returns on my investment?

There’s more to look at to get a more definitive answer to this question. But so far, what we’re seeing points to yes.

### Code Setup for Next Part

We will need to calculate p values for this next section of the analysis. P values are not a perfect metric: they can be tough to interpret and vulnerable to truth inflation. That said, we’ll do our best to take those things into account as we move along. Here you’ll see our method for getting a p value from some summary statistics.

In [ ]:

```plain text
import scipy.stats as stats

def p_value_for(num_samples_1, standard_deviation_1, mean1, num_samples_2, standard_deviation_2, mean2, confidence=0.95, num_comparisons=1):
  alpha = (1 - confidence)/num_comparisons
  total_degrees_freedom = num_samples_1 + num_samples_2 - 2

  degrees_freedom_1 = num_samples_1 - 1
  degrees_freedom_2 = num_samples_2 - 1
  combined_degrees_freedom = degrees_freedom_1 + degrees_freedom_2

  variance_1 = (standard_deviation_1 ** 2)
  variance_2 = (standard_deviation_2 ** 2)

  combined_variance_1 = (variance_1 / num_samples_1)
  combined_variance_2 = (variance_2 / num_samples_2)
  mean_standard_error = math.sqrt(combined_variance_1 + combined_variance_2)

  t_statistic = float(mean1 - mean2)/float(mean_standard_error)

  p_value = stats.t.sf(np.abs(t_statistic), combined_degrees_freedom) * 2  # two-sided p value = probability abs(t) > tt)
  comparison_adjusted_p_value = p_value/float(num_comparisons)
  return comparison_adjusted_p_value

```

I also want a better way to save, store, access, and analyze results for each ESG metric. So let’s make an object to store summary statistics, render plots, and run analyses by ESG metric. You’ll see the utility of this object in the following cells:

In [125]:

```plain text
class EsgMetricAnalysis():
    def __init__(self, name):
        self.name = name

        self.low_esg_metric_mean = 0
        self.low_esg_metric_std = 0
        self.low_esg_metric_num_examples = 0

        self.high_esg_metric_mean = 0
        self.high_esg_metric_std = 0
        self.high_esg_metric_num_examples = 0

        self.num_comparisons = 1

        self.esg_marker_sum_grouping = {}
        self.esg_marker_confidence_interval_data = {}
        self.esg_marker_bucket_group = {}
        self.esg_marker_bucket_data = {}

    def plots(self):
        f, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, sharex='col', sharey='row', figsize=(30, 20))
        f.suptitle('ESG Sums Plotted Against Returns')
        ax1.scatter(x=all_data[self.name + '_sum'], y=all_data['lower_conf_interval'])
        ax2.hist(x=all_data[self.name + '_sum'])
        ax3.plot(self.esg_marker_sum_grouping[self.name + '_sum'], self.esg_marker_sum_grouping['lower_conf_interval']['mean'])
        plot_grouping('Employment Strength Sums', self.esg_marker_confidence_interval_data, ax4)
        plot_grouping('Bucketed Employment Strength Sums', self.esg_marker_bucket_data, ax5)

    def t_test(self):
        return t_test_for(
        analysis.low_esg_metric_num_examples,
        analysis.low_esg_metric_std,
        analysis.low_esg_metric_mean,
        analysis.high_esg_metric_num_examples,
        analysis.high_esg_metric_std,
        analysis.high_esg_metric_mean,
        num_comparisons=self.num_comparisons
    )

    def p_value(self):
        return p_value_for(
        analysis.low_esg_metric_num_examples,
        analysis.low_esg_metric_std,
        analysis.low_esg_metric_mean,
        analysis.high_esg_metric_num_examples,
        analysis.high_esg_metric_std,
        analysis.high_esg_metric_mean,
        num_comparisons=self.num_comparisons
    )

```

OK, here is where we create all of our EsgMetricAnalysis objects. This method is messy, hard to interpret, and very dependent on the state of the notebook. I’d ultimately like to encapsulate this work in a series of pipelines and possibly extract a separate script for data preparation. That said, I promised to share incremental progress with you, and I have enough progress now that it’s time to share it.

In [ ]:

```plain text
def do_the_whole_dance_with(esg_metric=None, num_comparisons=1):
    analysis = EsgMetricAnalysis(esg_metric)

    esg_marker_data[esg_metric + '_sum'] = sum_scores_for(aggregate_data, esg_metric + '_sum')[esg_metric + '_sum']
    all_data[esg_metric + '_buckets'] = all_data.apply(lambda row: int(row[esg_metric + '_sum'] / 5), axis=1)

    analysis.esg_marker_sum_grouping = get_grouping_for(esg_metric + '_sum', all_data)
    analysis.esg_marker_confidence_interval_data = prepare_confidence_data_for(esg_metric + '_sum', analysis.esg_marker_sum_grouping)
    analysis.esg_marker_bucket_group = get_grouping_for(esg_metric + '_buckets', all_data)
    analysis.esg_marker_bucket_data = prepare_confidence_data_for(esg_metric + '_buckets', analysis.esg_marker_bucket_group)

    analysis.low_esg_metric_mean = all_data.sort_values(by=esg_metric + '_sum', ascending=False).iloc[0:20, :]['lower_conf_interval'].mean()
    analysis.low_esg_metric_std = all_data.sort_values(by=esg_metric + '_sum', ascending=False).iloc[0:20, :]['lower_conf_interval'].std()

    analysis.high_esg_metric_mean = all_data.sort_values(by=esg_metric + '_sum').iloc[0:20, :]['lower_conf_interval'].mean()
    analysis.high_esg_metric_std = all_data.sort_values(by=esg_metric + '_sum').iloc[0:20, :]['lower_conf_interval'].std()

    analysis.low_esg_metric_num_examples = 20
    analysis.high_esg_metric_num_examples = 20

    analysis.num_comparisons = num_comparisons

    return analysis

```

Let’s make sure our thing works:

In [136]:

```plain text
analysis = do_the_whole_dance_with('emp_str')
print(analysis.name)
print(analysis.p_value())

```

```plain text
emp_str
0.371418316064

```

Sweet! Now that we have created a way to run our analysis quickly on several ESG metrics, let’s try it on several of them at one time.

Ah! But when we do this, there’s something we have to watch out for: multiple comparisons! For each individual ESG metric, we are comparing the same examples. Remember that, if we test enough variables about our companies, eventually some of them will demonstrate a meaningful-looking effect by pure chance. A p-value of 0.05 still means a 5% probability of just such a chance occurrence, and when you do 100 comparisons the probability of at least one such false positive rises to…99%.

We are not making 99 comparisons, but we are making 11. For 11 comparisons at a p value of 0.05, the probability of a meaningful-looking result occurring by chance is…

In [50]:

```plain text
1-(0.95**11)

```

Out[50]:

```plain text
0.43119990772354033
```

…0.43. That is, there’s a 43% chance we get a false positive. That’s a fairly high probability.

There are a few ways to adjust our calculations for multiple comparisons. The most hamfisted of these, the Bonferroni Correction, adjusts by dividing our starting p value by the number of comparisons we make and using that as the working p-value.

The Bonferroni correction controls the familywise error rate—the probability, assuming all the variables have identical distribution in the two groups, that a significant-looking difference happens purely by chance.

So in this case, our working p value would be…

In [52]:

```plain text
0.05 /11

```

Out[52]:

```plain text
0.004545454545454546
```

…0.004. Very, very tiny.

Unsurprisingly, when we run our analysis with the Bonferroni correction, we do not find a single statistically significant difference in stock prices among the lot. Check it out:

In [129]:

```plain text
esgs = [
    'emp_str',
    'emp_con',
    'env_str',
    'env_con',
    'alc_con',
    'cgov_str',
    'cgov_con',
    'com_str',
    'com_con',
    'div_str',
    'div_con'
]

analyses = []
for esg in esgs:
    analyses.append(do_the_whole_dance_with(esg, num_comparisons=11))

```

That is a lot of ‘true’values for accepting the null hypothesis.

We can also check our resuts with the Benjamini-Hochberg procedure.

The Benjamini-Hochberg correction controls the False discovery rate—the expected proportion of false positives among the variables for which you claim the existence of a difference. For example, if with FDR controlled to 5% 20 tests are positive, on average one of these tests will be a false positive (because 1 in 20 is 5%).

To run this tesst, we choose a false positive rate (Q) to which we wish to control. Let’s choose 10% to start and see what we come up with.

In [176]:

```plain text
p_values = {}
for esg in esgs:
    analysis = do_the_whole_dance_with(esg) #We leave num_comparisons at 1 since we're about to correct for it with a different procedure.
    p_values[analysis.name] = analysis.p_value()

p_values

```

Out[176]:

```plain text
{'alc_con': 0.52149743027205497,
 'cgov_con': 0.12443279432304483,
 'cgov_str': 0.023657375782642983,
 'com_con': 0.60932126282257471,
 'com_str': 0.029299680869613209,
 'div_con': 0.33570341442667906,
 'div_str': 0.086155730251918514,
 'emp_con': 0.42023821025024821,
 'emp_str': 0.37141831606394926,
 'env_con': 0.1098226363187968,
 'env_str': 0.58368331428660714}
```

How to do this test:

1. Rank all your p values lowest to highest.
1. Calculate (Q * rank)/number of comparisons.
1. Go down the list until the calculated value Qi/n is lower than p.
In [177]:

```plain text
benjamini_hochberg_data = pd.DataFrame({'name': list(p_values.keys()), 'p_value': list(p_values.values())})
benjamini_hochberg_data = benjamini_hochberg_data.sort_values(by='p_value')
benjamini_hochberg_data['rank'] = range(11)
benjamini_hochberg_data['rank'] = benjamini_hochberg_data['rank'] + 1
benjamini_hochberg_data['Qi/n'] = (0.1 * benjamini_hochberg_data['rank']) / float(11)
benjamini_hochberg_data

```

Out[177]:

Check it out: even on the very first one, the p value is larger than Qi/n. So when we analyze with this procedure, we’re still not finding a meaningful difference between stock performances based on company ESG score.

### Like this:

April 12, 2018 Chelsea

Reading Time:  17 minutes

WARNING: This post speaks frankly about race and gender dynamics. I use the term ‘white men’ on several occasions. If that feels to you like a personal attack, I do not recommend this blog post for you right now. Instead, I recommend this resource.

If the warning applies to you and you insist on knowing what I have to say, I recommend skipping to the conclusion.

Inside your soul, you value diversity. But at this company that you’ve built/joined, the employees and/or leadership are overwhelmingly white and male.

And somehow, even though you feel like you are trying, the people joining your team also seem to be overwhelmingly white and male.

What the heck is going on? And what can you do to build a more diverse team?

It’s true that diversity is not a pipeline problem. At the industry level, it is chiefly an inclusion problem tied to very specific monetary outcomes for your company.

But on a company level, it’s very hard for you to credibly claim you’re inclusive if you have a homogeneous staff (and no, the line about ‘diversity of thought’ isn’t convincing anyone). So at some point you gotta try to source and hire queer people, trans people, black people, latinx people, women.*

- These are examples. There are many, many axes of oppression, and this blog post doesn’t mention them all. Accessibility for disabled folks is not mentioned. Hiring neurodiverse folks is not mentioned. The bamboo ceiling is not mentioned. These things are also important. I highly recommend seeking out other resources in addition to this one to develop a well-rounded understanding of intersectionality.
Before we move on, let’s level set. This post assumes that you already have some knowledge about common pipeline mistakes that funnel a homogeneous applicant pool into your company. It assumes that you already know that advertising in mainstream tech spaces will overwhelmingly place the ad in front of dominant groups. It assumes that you know that how you write your job descriptions causes folks from underrepresented groups to self-select out. It assumes that you know that folks from underrepresented groups will see your experience requirements as requirements, while most white men will instead view them as suggestions. It assumes that you know that bias in interviews affects who is getting through your pipeline. If any of these things are news to you, I recommend doing some additional research and coming back later when you have a deeper understanding of the dynamics that affect who you see and how you see them during the hiring process. When in doubt, I often recommend this repo as a great place to start.

So let’s say you know all that, and now you’re trying to specifically reach out to a more diverse applicant pool—say, by running a hiring ad in a listserv for women in tech, or by sponsoring meetups for underrepresented groups. Let’s address why those approaches might not be yielding the results you would like. We’ll also use that knowledge to identify some additional strategies that can help you make the most of your recruiting efforts.

### What if we run an ad in [insert underrepresented people in tech listserv here]?

No doubt, this will get your name in front of said groups. Know this:

The individuals you’re looking for are screening you.

And the more senior they are, the more they are screening you. They want to know if you have queer people, trans people, black people, latinx people, women in leadership. They want to know if you place those people in senior roles. They’re looking at the management chain above them: is it white dudes all the way up?

Turtles stacked on top of each other. The top turtles all have the white man emoji over their faces. The bottom turtle has a woman of color emoji face.

Outspoken and experienced women, for example, are screening because they’re sick of being the first woman on the engineering team and suddenly becoming the sole delegate for the ‘woman’s perspective’ on things. They’re sick of being assumed less technical and getting forced to prove themselves again and again and again. They’re sick of being the one expected to buy the thank you cards for clients, to remember people’s birthdays, to order lunch for the team. And they’re sick of showing up to a place that’s ‘trying to be more diverse’ and getting slapped with the term ‘aggressive’ the first time they call out these problematic dynamics. Because once people think a woman is ‘aggressive’ she won’t get promoted. Would you stay in that situation? She didn’t the last time this happened to her, and she doesn’t want to have it happen again.

At your company, she would be surrounded and superiored by people who have no lived experience of marginalization. This means that your company displays a risk factor for placing people in situations like that. So you’re getting screened out.

You can run ads to get name recognition from these groups, but if you don’t pass their screens, they’re not going to reach out to you. The more senior the candidates, the more true this is because senior tech people have more options. And senior matters to you because once you have queer people, trans people, black people, latinx people, and women in CXO, VP, leadership, and senior roles, you start passing the screens that you fail while it’s white dudes all the way up.

Hiring junior folks from underrepresented groups is important, but it will do little to help you pass the screens of future applicants because:

a) junior people do not have the hard power to change things that are problematic

b) having junior people from underrepresented groups does not say anything about leadership’s willingness to listen to diverse perspectives. This is because leadership has no hierarchal obligation to listen to junior people. Shrewd junior people, aware of this power dynamic, are less likely to raise their voices in the first place.

### What if we sponsor events for [insert underrepresented people in tech meetup here] ?

Again, this will get your name in front of said groups. Again, things to know:

Thing #1: These groups are screening you. Some groups are explicit about this. For example, Chicago Women Developers does not let dudes come ‘mentor’ at their hack nights. They also have you answer several questions about inclusion practices at your company before they will share your job posting with members. Even if a group doesn’t explicitly ask these questions, their members often are. If a company broadcasts that they’re hiring to a black coding group, the people in that group are looking to see if they would have any black colleagues there. If not, the more senior ones will not apply, and the more junior ones will be less likely to apply.

Thing #2: tech meetups skew junior. A few reasons why:

- First, people with more years of experience tend to have more years period, which often come with families and other obligations that keep them off the meetup circuit. This is especially true for women, who often shoulder more childcare responsibilities than their husbands.
- Second, the preponderance of young people at these events as well as the preponderance of beer-and-pizza pot sweeteners creates an environment rife for harassment, so women, queer people, and trans people who have experienced that harrassment avoid meetups so they don’t have to repeat that. This is still true in meetups for underrepresented groups; just because it’s for queer people doesn’t mean trans people won’t face harassment there, and just because it’s for people of color doesn’t mean women won’t face harassment there (that’s right: intersectionality is complicated).
- Third, a lot of junior folks attend these meetups to network for jobs or find mentors. As folks start to build their networks and experience, the meetups are less useful to them, so they show up less. The waning group of senior attendees find themselves shouldering expectations of connections, jobs, and mentorship from the much larger number of junior attendees. The senior attendees do not have time or energy to fulfill all of these expectations, so they might stop coming to avoid inevitably disappointing people since they cannot provide enough resources to go around.
Junior tech people are an opportunity, not a problem. But junior tech people, as previously explained, are not in a power position to shift your culture on their own. And for that reason, junior people also do not shift your results when you get screened by more senior applicants.

### So what can you do?

Your #1 force multiplier for building out a diverse pipeline is:

Getting queer people, trans people, black people, latinx people, and women into your boardroom.

And CTO. And CDO. And CSO. And Senior VP. And VP. And Director. And Manager. And Senior. Technical roles, not “Chief Culture Officer.” If you have a high-level role to fill, prioritize people who can fill your gaps up top.

But Chelsea, it’s illegal to discriminate on the basis of race or gender…I think it’s really interesting how this legal protection only ever gets mentioned when we’re talking about passing on white guys’ applications. I’ve never heard someone bring this up in defense of a candidate from an underrepresented group. Somehow there’s always some other reason, right? Sometimes the reason is as vague as ‘not a good fit for our team.’ What the hell does that mean? If that counts as a valid reason for you to pass on someone without being concerned about a discrimination suit, then discrimination becomes un-prosecutable because you could claim ‘not a good fit’ about literally anyone. To entertain the position that someone will sue us if we don’t hire more white guys is patently absurd.

Look around your boardroom. You’re gonna try tell me that the demographic represented by the vast majority of the room is discriminated against? I get it, you feel like thinking about these things during hiring equals discrimination. Here’s the thing: not thinking about these things during hiring equals discrimination. The status quo is discrimination—and it’s discrimination against the exact groups you say you want to attract. The proof is in the photo from your last leadership retreat. You’re not a fluke where this is just who happened to get hired and promoted. How do I know that? Because your leadership retreat photo looks the same as the leadership retreat photo for 90% of tech companies. 90% of thousands of companies do not independently experience flukes. The probability of that is so infinitesimally small that to entertain it is, again, patently absurd.

I’m not telling you to hire a CXO specifically because they’re black. I’m telling you that there is a critical weakness in the collective perspective of a homogeneous leadership, and it’s your responsibility to hire team members who supplement the team’s weaknesses. If you hire more team members who also lack the perspective that your leadership team already lacks, you’re indicating that that weakness is not critical to you. That’s your company’s opinion, maybe, but it’s at odds with the opinions of the people you’re trying to hire. And that weakness will prevent you from being a competitive employment option for the slice of the talent pool that screens you.

### How do you attract these senior people with all this screening going on?

Ads and meetups are great, but they won’t get you the applicants you’re looking for if you’re failing the screen.

If you cannot pass the screen, you have to get around it.

A few helpful tactics:

1. Reach out directly. send them an email. Address them by name, mention how you found out about them, and explain why you want them specifically to work for you. What background do they have that you want? Not just queerness: I mean their technical background. I was not hired into my current role because I am a queer lady. I was hired into it because I have the chops to do the job.

Yes, I know it’s a lot of work to individually contact people like this. Most of the time, you’re going to get a ‘Thanks, but I’m happy here’ reply.

So let me level with you: you and every other company in tech are all after a pretty small group of people, and that group of people is pretty skeptical of you specifically based on past experiences with companies that look a lot like yours. This is very much an uphill slog. You’re going to have to climb.

Sidenote on this: do not end the message with “and if you’re not interested, please send along the names of anyone else who might be!” a) This person does not know you. Why are they gonna give you free access to their network that they spent time, energy, and money building, and in the process subject their friends to unsolicited contact from a company that did not pass the screen? b) This makes your explanation of why you want them specifically seem disingenuous, because you capped it off with “also please send along literally anybody else.”

2. Be prepared to answer hard questions. What percentage of your leadership is (insert some group you have none of on your leadership team)? What is your company’s policy around (something you’ve never heard of)? In performance reviews, how do you control for (something you have never considered)? These, by the way, are the easy questions. They get a lot harder when you bring these folks on. And you want them to ask these questions, because finding good answers is how your team gets better. It’s not annoying or burdensome or “stirring the pot” or whatever that old white turtle man on your board is telling you it is: it’s an important method by which these candidates add value to your team.

In the meantime, be prepared to do two things:

- Give straight answers, even when they are not flattering to your company. You don’t have to say “Well, there are no latinx people on our leadership team, but….” You can stop at “Currently, zero. We recognize that our collective perspective as a leadership team has a weakness because of this.” (Sidenote: do not end this answer with “You can be the first!” unless it is, in fact, the role you are hiring them for. Women ICs who have been told they might be promoted to “first” woman manager someday often see themselves passed up for that exact promotion by many, many dudes once they join).
- Explain your current strategy to get better.  
The exact things you’re doing matter, because here’s the dirty secret: you are going to have unflattering answers to most screening questions, and so are 90% of the companies you’re competing with to get this person. So you can beat them by having a better plan in place to address your weaknesses.

3. Demonstrate that when you say you want them to come, you mean it. Folks from underrepresented groups end up with additional uncompensated representation labor and emotional labor in mostly-homogeneous teams. So compensate them for it. Benefits, remote work, time off, their pick of managers, whatever it takes to win this person over.

And pay them good money. Do not try to skate off with the lowest salary they’ll take. Do not ask them any variant of the question “What is the lowest amount you’d come to us for?” Give them the top amount a person of their experience would make.

And, for the love of God, don’t play hardball. I know, you saw it in a movie and it looked badass, whatever. But keep in mind that a) you, like, need this person, and b) people who have experienced a chronic pattern of getting less than their due in this industry absolutely hate hardball. Maybe they go along with it because they feel like they have to, but they won’t forget that you did it, and they might kinda resent you. What you don’t need in your life is to get a head start on filling up this person’s list of reasons to leave.

Exercise what you know about consent: extracting a reluctant or stressed out ‘yes’ from someone is not consent. There’s an enthusiastic “yes” that someone came to on their own terms, and there’s “no.” Those are the two possible answers when you care about a long-term relationship with a person. Don’t surrender to the glitz of negotiation myopia.

But what if we overpay them by accident? You’re much less likely to overpay them by accident than you are to overpay someone from the dominant group by accident. An outsize proportion of promoted-out-of-harm’s-way do-nothings sucking up multiple six figures that I have met in business have been from the same demographic. I’ll let you guess which one it is. So if you’re not worried about this for your ‘usual’ hire, then you have absolutely nothing to worry about when it comes to hiring the people you are specifically and strategically targeting.

This post is pretty candid, so it’s totally cool if you need to take a minute, get a tea, have a think. When you’re done, this piece describes one really valuable systematic effort you can make to improve inclusion at your company. At time of writing, it was the most-read piece I’ve ever written, and it has been used to influence the hiring an advancement process at places like Betterment, Mozilla, one specific team at Apple, and about a dozen Chicago tech companies that I know of.

### Conclusion

If your company is homogeneous, how can you hire more diverse folks? Maybe you’ve tried targeting underrepresented groups with your recruitment ads and it doesn’t seem to be working. Why not?

It’s not working because folks from underrepresented groups run initial screens on potential employers. Those screens look for people like them in leadership positions above them. If they don’t see that, they’re less likely to reach out to you. And the more senior they are, the more options they have, so their likeliness to reach out drops even further.

This creates a Catch-22: without people from underrepresented groups in leadership, you cannot pass screens run by leadership-level people from underrepresented groups. What do you do?

You have to get around the screens. I recommend, first of all, prioritizing better representation at the top of your company so you can pass future screens and start getting a return on your investment in things like job listservs and meetups.

But to get those key people for top positions at your company, you’ll need to get around their screens. Reach out to them individually, find out what they’d bring to your company, and sell them on what their specific future at your company would look like. Answer their hard questions honestly, and leave out the excuses. Be prepared to share your specific action items for fostering an inclusive culture at your company as well as your progress to date on those action items. Finally, make them an initial offer that proves you’re very, very serious about bringing them on.

If you’re worried about discriminating or overpaying, I recommend you consider why you’re so worried about these things when it comes to underrepresented groups and not worried about those things when you’re bringing on another white dude. The default in our system is discrimination. So we have to think about these things in the hiring process in order to not discriminate.

### If you learned something from this post and you’re not totally incensed, you might also want to see these:

Anger and Sadness in the Workplace

Smart is Not a Hiring Criterion

Bias Doesn’t Start with Skin Color


