---
type: link
source: notion
url: https://simonwillison.net/2024/Dec/11/gemini-2/
notion_type: Tech Announcement
tags: ['Running']
created: 2024-12-12T00:54:00.000Z
---

# Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode

## Overview (from Notion)
- Impact of AI on Work: The advancements in Gemini 2.0 Flash could streamline your software development processes, allowing for more efficient coding and debugging through its code execution capabilities.
- Multimodal Interaction: The ability to process and output various modalities (text, audio, images) opens up new avenues for app development, possibly enhancing user experience in your products.
- AI in Daily Life: Consider using AI for personal tasks, such as meal planning or helping your kids with homework, making daily life more manageable.
- Future of Communication: The streaming capabilities could redefine remote work, enabling real-time collaboration and project discussions using audio and video inputs.
- Family Engagement: Leverage AI to create engaging educational content for your children, fostering their curiosity and learning in a fun way.
- Alternative Views: While advancements are exciting, there are concerns about AI ethics, job displacement, and data privacy that warrant consideration.
- Skepticism: Not everyone believes in the positive potential of these technologies; some experts raise alarms about dependency on AI and its implications for critical thinking and creativity.
- Prepare for Change: Staying informed about these developments will allow you to adapt your business strategies and personal life to harness the benefits while mitigating risks.

## AI Summary (from Notion)
Gemini 2.0 Flash is a new multi-modal LLM from Google, offering faster performance and the ability to process and output images, audio, and text. It features impressive streaming capabilities, can write and execute code, and introduces advanced spatial understanding with bounding boxes. Upcoming features include enhanced image editing and audio output with varied voices and accents.

## Content (from Notion)

Huge announcment from Google this morning: Introducing Gemini 2.0: our new AI model for the agentic era. There’s a ton of stuff in there (including updates on Project Astra and the new Project Mariner), but the most interesting pieces are the things we can start using today, built around the brand new Gemini 2.0 Flash model. The developer blog post has more of the technical details.

Gemini 2.0 Flash is a multi-modal LLM. Google claim it’s both more capable and twice as fast as Gemini 1.5 Pro, their previous best model.

The new Flash can handle the same full range of multi-modal inputs as the Gemini 1.5 series: images, video, audio and documents. Unlike the 1.5 series it can output in multiple modalities as well—images and audio in addition to text. The image and audio outputs aren’t yet generally available but should be coming early next year.

It also introduces streaming capabilities which are wildly impressive. More on that later in this post.

- Running a vision prompt using llm-gemini;
- Impressive performance on bounding boxes
- It can both write and execute code
- The streaming API is next level
- Things to look forward to
Let’s start by trying it out as a multi-modal input, text output model.

### Running a vision prompt using llm-gemini; #

I released llm-gemini 0.7 adding support for the new model to my LLM command-line tool. You’ ll need a Gemini API key—then install LLM and run:

```plain text
llm install -U llm-gemini
llm keys set gemini
# ... paste API key here
llm -m gemini-2.0-flash-exp describe \
  -a https://static.simonwillison.net/static/2024/pelicans.jpg
```

That’s using my pelicans.jpg image as input, a low resolution photograph of a confusing mass of pelicans on a rocky outcrop. It’s a good starting point for exploring a model’s vision capabilities.

A confusing photo of pelicans on rocks

The full transcript is here. This is one of the best results I’ve seen:

> 

A white head on a California Brown Pelican is breeding plumage, which does indeed indicate they are older birds.

### Impressive performance on bounding boxes #

One of the most interesting characteristics of the Gemini 1.5 series is its ability to return bounding boxes for objects within an image. I described a tool I built for exploring that in Building a tool showing how Gemini Pro can return bounding boxes for objects in images back in August.

I upgraded that tool to support Gemini 2.0 Flash and ran my pelican photo through, with this prompt:

> 

Here’s what I got back, overlayed on the image by my tool:

Given how complicated that photograph is I think this is a pretty amazing result.

AI Studio now offers its own Spatial Understanding demo app which can be used to try out this aspect of the model, including the ability to return “3D bounding boxes” which I still haven’t fully understood!

Prompt is Detect the 3D bounding boxes of pelicans , output no more than 10 items. Output a json list where each entry contains the object name in "label" and its 3D bounding box in "box_3d" - the model has returned a JSON list like that and is showing the photograph of pelicans on the right covered in exciting 3D boxes, which look like they about line up with the 2D pelicans

Google published a short YouTube video about Gemini 2.0’s spatial understanding.

### It can both write and execute code #

The Gemini 1.5 Pro models have this ability too: you can ask the API to enable a code execution mode, which lets the models write Python code, run it and consider the result as part of their response.

Here’s how to access that using LLM, with the -o code_execution 1 flag:

```plain text
llm -m gemini-2.0-flash-exp -o code_execution 1 \
  'write and execute python to generate a 80x40 ascii art fractal'
```

The full response is here—here’s what it drew for me:

The code environment doesn’t have access to make outbound network calls. I tried this:

```plain text
llm -m gemini-2.0-flash-exp -o code_execution 1 \
  'write python code to retrieve https://simonwillison.net/ and use a regex to extract the title, run that code'
```

And the model attempted to use requests, realized it didn’t have it installed, then tried urllib.request and got a Temporary failure in name resolution error.

Amusingly it didn’t know that it couldn’t access the network, but it gave up after a few more tries.

### The streaming API is next level #

The really cool thing about Gemini 2.0 is the brand new streaming API. This lets you open up a two-way stream to the model sending audio and video to it and getting text and audio back in real time.

I urge you to try this out right now using https://aistudio.google.com/live. It works for me in Chrome on my laptop and Mobile Safari on my iPhone—it didn’t quite work in Firefox.

Here’s a minute long video demo I just shot of it running on my phone:

Play: Gemini 2.0 streaming demo

Embed: <https://www.youtube-nocookie.com/embed/mpgWH9KulUU?autoplay=1&playsinline=1>

The API itself is available to try out right now. I managed to get the multimodal-live-api-web-console demo app working by doing this:

1. Clone the repo: git clone https://github.com/google-gemini/multimodal-live-api-web-console
1. Install the NPM dependencies: cd multimodal-live-api-web-console && npm install
1. Edit the .env file to add my Gemini API key
1. Run the app with npm start
It’s pretty similar to the previous live demo, but has additional tools—so you can tell it to render a chart or run Python code and it will show you the output.

This stuff is straight out of science fiction: being able to have an audio conversation with a capable LLM about things that it can “see” through your camera is one of those “we live in the future” moments.

Worth noting that OpenAI released their own WebSocket streaming API at DevDay a few months ago, but that one only handles audio and is currently very expensive to use. Google haven’t announced the pricing for Gemini 2.0 Flash yet (it’s a free preview) but if the Gemini 1.5 series is anything to go by it’s likely to be shockingly inexpensive.

### Things to look forward to #

I usually don’t get too excited about not-yet-released features, but this thing from the Native image output video caught my eye:

On teh left a photo of a mini. A prompt says Turn this car into a convertible and ond the right is that car but now as a convertible.

The dream of multi-modal image output is that models can do much more finely grained image editing than has been possible using previous generations of diffusion-based image models. OpenAI and Amazon have both promised models with these capabilities in the near-future, so it looks like we’re going to have a lot of fun with this stuff in 2025.

The Building with Gemini 2.0: Native audio output demo video shows off how good Gemini 2.0 Flash will be at audio output with different voices, intonations, languages and accents. This looks similar to what’s possible with OpenAI’s advanced voice mode today.

Posted 11th December 2024 at 8:16 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter

## More recent articles

- ChatGPT Canvas can make API requests now, but it's complicated - 10th December 2024
- I can now run a GPT-4 class model on my laptop - 9th December 2024
This is Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode by Simon Willison, posted on

google 333   ai 957   generative-ai 815   llms 810   gemini 45   vision-llms 33 


